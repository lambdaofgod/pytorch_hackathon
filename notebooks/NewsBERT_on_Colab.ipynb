{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NewsBERT on Colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrRtlUpXmOcC",
        "colab_type": "text"
      },
      "source": [
        "# NewsBERT on Colab\n",
        "\n",
        "This notebook uses awesome [streamlit tutorial](https://youtu.be/x0NdZkaciws) to run NewsBERT on Colab.\n",
        "\n",
        "For running this code you will need to setup your [ngrok](https://ngrok.com/) account"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puIeoSMDmNzJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install streamlit pyngrok\n",
        "!pip install git+https://github.com/lambdaofgod/pytorch_hackathon/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdjyd9NdZ2v9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install torch==1.5.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lqz1wl14aIYa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1b26abaa-b69f-425d-a532-2bfc4ff0a71f"
      },
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95E2S3CMQNgo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b332055e-a7a1-48b0-b7cd-50e163ce13d3"
      },
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tqdm\n",
        "import os\n",
        "from operator import itemgetter\n",
        "\n",
        "import torch\n",
        "from pytorch_hackathon import rss_feeds, zero_shot_learning, haystack_search\n",
        "import seaborn as sns\n",
        "\n",
        "st.title('Zero-shot RSS feed article classifier')\n",
        "\n",
        "cm = sns.light_palette(\"green\", as_cmap=True)\n",
        "topic_strings = list(pd.read_table('https://raw.githubusercontent.com/lambdaofgod/pytorch_hackathon/master/data/topics.txt', header=None).iloc[:,0].values)\n",
        "rss_feed_urls = list(pd.read_table('https://raw.githubusercontent.com/lambdaofgod/pytorch_hackathon/master/data/feeds.txt', header=None).iloc[:,0].values)\n",
        "rss_feed_urls = rss_feeds.rss_feed_urls.copy()\n",
        "\n",
        "\n",
        "model_device = st.selectbox(\"Model device\", [\"cpu\", \"cuda\"], index=int(torch.cuda.is_available()))\n",
        "\n",
        "\n",
        "@st.cache(allow_output_mutation=True)\n",
        "def get_feed_df():\n",
        "    with st.spinner('Retrieving articles from feeds...'):\n",
        "        return rss_feeds.get_feed_df(rss_feed_urls)\n",
        "\n",
        "\n",
        "feed_df = get_feed_df()\n",
        "\n",
        "\n",
        "@st.cache(allow_output_mutation=True)\n",
        "def setup_searcher(feed_df, use_gpu, model_name=\"deepset/sentence_bert\"):\n",
        "    with st.spinner('No precomputed topics found, running zero-shot learning...'):\n",
        "        searcher = haystack_search.Searcher(model_name, 'text', use_gpu=use_gpu)\n",
        "        searcher.add_texts(feed_df)\n",
        "    return searcher \n",
        "\n",
        "\n",
        "# we need to copy feed_df so that streamlit doesn't recompute embeddings when feed_df changes \n",
        "searcher = setup_searcher(feed_df, use_gpu=model_device == 'cuda') \n",
        "\n",
        "\n",
        "@st.cache\n",
        "def get_retrieved_df(topic_strings):\n",
        "    results = [\n",
        "        result \n",
        "        for topic in topic_strings\n",
        "        for result in searcher.retriever.retrieve(\n",
        "            \"text is about {}\".format(topic)\n",
        "        )\n",
        "    ]\n",
        "    return searcher.get_topic_score_df(\n",
        "        results,\n",
        "        topic_strings\n",
        "    ).drop_duplicates(subset='title')\n",
        "    \n",
        "\n",
        "selected_df = get_retrieved_df(topic_strings).reset_index(drop=True)\n",
        "selected_df['text'] = selected_df['text'].apply(lambda s: s[:1000])\n",
        "topics = st.multiselect('Choose topics', topic_strings, default=[topic_strings[0]])\n",
        "sort_by = st.selectbox(\"Sort by\", topics)\n",
        "display_df = selected_df[selected_df[topics].min(axis=1) > 0.5].sort_values(sort_by, ascending=False)\n",
        "\n",
        "st.markdown('## Articles on {}'.format(', '.join(topics)))\n",
        "\n",
        "st.table(display_df[display_df[topics].min(axis=1) > 0.5].style.background_gradient(cmap=cm))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1laW7BxnDY0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Checking if GPU is available\n",
        "\n",
        "## Running this on GPU will be much faster (embeddings for articles will be calculated much faster)."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8Fve21IZIYT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isXI-T7zTPYn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyngrok import ngrok"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6yG4G9OmtGH",
        "colab_type": "text"
      },
      "source": [
        "# ngrok token\n",
        "\n",
        "You will need to paste your ngrok authtoken. You can find it [here](https://dashboard.ngrok.com/auth/your-authtoken) provided you have an ngrok account."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGUrrsG1T4Oj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8065d3b9-1a9a-4d2f-d235-95ae1b973f9b"
      },
      "source": [
        "#@title ngrok token\n",
        "\n",
        "token = '' #@param {type:\"string\"}\n",
        "!ngrok authtoken $token"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJKlMN25UEpp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!streamlit run app.py&>log&"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKeo80BSU5l6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pub_port = !cat log | grep \"Network URL:\" | awk -F\":\" '{print $4}'\n",
        "pub_port = int(pub_port[0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX4w3S7ig7Z3",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqhXsfo7T_jA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "publ_url = ngrok.connect(pub_port)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfBe34a9UKDH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "publ_url"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhHfmy51flHA",
        "colab_type": "text"
      },
      "source": [
        "If application fails to start try to close all existng ngrok tunnel ang bo back to this [cell](https://colab.research.google.com/drive/1oJagYsBBfGugVdo5fuY4Fw5hZP6uUQo7#scrollTo=RKeo80BSU5l6&line=1&uniqifier=1) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSbfQUryfgCw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pkill streamlit\n",
        "tunnels = ngrok.get_tunnels()\n",
        "for tunnel in tunnels:\n",
        "  ngrok.disconnect(tunnel.public_url)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}