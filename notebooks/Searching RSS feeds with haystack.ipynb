{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp haystack_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "from nltk import tokenize\n",
    "from operator import itemgetter\n",
    "\n",
    "import haystack\n",
    "from haystack import database\n",
    "import haystack.database.memory\n",
    "\n",
    "from haystack.retriever.dense import EmbeddingRetriever\n",
    "from pytorch_hackathon import rss_feeds\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.disable(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = sns.light_palette(\"green\", as_cmap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/piotr/Documents/pytorch_hackathon\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feeds.txt  topics.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rss_feed_urls = list(pd.read_table('data/feeds.txt', header=None).iloc[:,0].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:08<00:00,  1.78it/s]\n",
      "/home/piotr/Documents/pytorch_hackathon/pytorch_hackathon/rss_feeds.py:64: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 64 of the file /home/piotr/Documents/pytorch_hackathon/pytorch_hackathon/rss_feeds.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  feed_df['text'] = feed_df['summary'].apply(lambda s: bs4.BeautifulSoup(s).text)\n"
     ]
    }
   ],
   "source": [
    "feed_df = rss_feeds.get_feed_df(rss_feed_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>title_detail</th>\n",
       "      <th>links</th>\n",
       "      <th>link</th>\n",
       "      <th>summary</th>\n",
       "      <th>summary_detail</th>\n",
       "      <th>id</th>\n",
       "      <th>guidislink</th>\n",
       "      <th>tags</th>\n",
       "      <th>text</th>\n",
       "      <th>...</th>\n",
       "      <th>comments</th>\n",
       "      <th>authors</th>\n",
       "      <th>author</th>\n",
       "      <th>author_detail</th>\n",
       "      <th>updated</th>\n",
       "      <th>updated_parsed</th>\n",
       "      <th>content</th>\n",
       "      <th>href</th>\n",
       "      <th>media_thumbnail</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pose2Mesh: Graph Convolutional Network for 3D Human Pose and Mesh Recovery from a 2D Human Pose</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base': 'https://us-east1-ml-feeds.cloudfunctions.net/p...</td>\n",
       "      <td>[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://paperswithcode.com/paper/pose2mesh-g...</td>\n",
       "      <td>https://paperswithcode.com/paper/pose2mesh-graph-convolutional-network-for-3d</td>\n",
       "      <td>Most of the recent deep learning-based 3D human pose and mesh estimation methods regress the pos...</td>\n",
       "      <td>{'type': 'text/html', 'language': None, 'base': 'https://us-east1-ml-feeds.cloudfunctions.net/pw...</td>\n",
       "      <td>https://paperswithcode.com/paper/pose2mesh-graph-convolutional-network-for-3d</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Most of the recent deep learning-based 3D human pose and mesh estimation methods regress the pos...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-08-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Uncertainty Estimation in Medical Image Denoising with Bayesian Deep Image Prior</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base': 'https://us-east1-ml-feeds.cloudfunctions.net/p...</td>\n",
       "      <td>[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://paperswithcode.com/paper/uncertainty...</td>\n",
       "      <td>https://paperswithcode.com/paper/uncertainty-estimation-in-medical-image</td>\n",
       "      <td>We use a randomly initialized convolutional network as parameterization of the reconstructed ima...</td>\n",
       "      <td>{'type': 'text/html', 'language': None, 'base': 'https://us-east1-ml-feeds.cloudfunctions.net/pw...</td>\n",
       "      <td>https://paperswithcode.com/paper/uncertainty-estimation-in-medical-image</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'term': 'Image denoising', 'scheme': None, 'label': None}]</td>\n",
       "      <td>We use a randomly initialized convolutional network as parameterization of the reconstructed ima...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-08-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yet Another Intermediate-Level Attack</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base': 'https://us-east1-ml-feeds.cloudfunctions.net/p...</td>\n",
       "      <td>[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://paperswithcode.com/paper/yet-another...</td>\n",
       "      <td>https://paperswithcode.com/paper/yet-another-intermediate-level-attack</td>\n",
       "      <td>The transferability of adversarial examples across deep neural network (DNN) models is the crux ...</td>\n",
       "      <td>{'type': 'text/html', 'language': None, 'base': 'https://us-east1-ml-feeds.cloudfunctions.net/pw...</td>\n",
       "      <td>https://paperswithcode.com/paper/yet-another-intermediate-level-attack</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The transferability of adversarial examples across deep neural network (DNN) models is the crux ...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-08-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lite Training Strategies for Portuguese-English and English-Portuguese Translation</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base': 'https://us-east1-ml-feeds.cloudfunctions.net/p...</td>\n",
       "      <td>[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://paperswithcode.com/paper/lite-traini...</td>\n",
       "      <td>https://paperswithcode.com/paper/lite-training-strategies-for-portuguese</td>\n",
       "      <td>Despite the widespread adoption of deep learning for machine translation, it is still expensive ...</td>\n",
       "      <td>{'type': 'text/html', 'language': None, 'base': 'https://us-east1-ml-feeds.cloudfunctions.net/pw...</td>\n",
       "      <td>https://paperswithcode.com/paper/lite-training-strategies-for-portuguese</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'term': 'Machine translation', 'scheme': None, 'label': None}]</td>\n",
       "      <td>Despite the widespread adoption of deep learning for machine translation, it is still expensive ...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-08-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Simultaneous Detection and Tracking with Motion Modelling for Multiple Object Tracking</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base': 'https://us-east1-ml-feeds.cloudfunctions.net/p...</td>\n",
       "      <td>[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://paperswithcode.com/paper/simultaneou...</td>\n",
       "      <td>https://paperswithcode.com/paper/simultaneous-detection-and-tracking-with</td>\n",
       "      <td>Deep learning-based Multiple Object Tracking (MOT) currently relies on off-the-shelf detectors f...</td>\n",
       "      <td>{'type': 'text/html', 'language': None, 'base': 'https://us-east1-ml-feeds.cloudfunctions.net/pw...</td>\n",
       "      <td>https://paperswithcode.com/paper/simultaneous-detection-and-tracking-with</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'term': 'Multiple object tracking', 'scheme': None, 'label': None}]</td>\n",
       "      <td>Deep learning-based Multiple Object Tracking (MOT) currently relies on off-the-shelf detectors f...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-08-25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                             title  \\\n",
       "0  Pose2Mesh: Graph Convolutional Network for 3D Human Pose and Mesh Recovery from a 2D Human Pose   \n",
       "1                 Uncertainty Estimation in Medical Image Denoising with Bayesian Deep Image Prior   \n",
       "2                                                            Yet Another Intermediate-Level Attack   \n",
       "3               Lite Training Strategies for Portuguese-English and English-Portuguese Translation   \n",
       "4           Simultaneous Detection and Tracking with Motion Modelling for Multiple Object Tracking   \n",
       "\n",
       "                                                                                          title_detail  \\\n",
       "0  {'type': 'text/plain', 'language': None, 'base': 'https://us-east1-ml-feeds.cloudfunctions.net/p...   \n",
       "1  {'type': 'text/plain', 'language': None, 'base': 'https://us-east1-ml-feeds.cloudfunctions.net/p...   \n",
       "2  {'type': 'text/plain', 'language': None, 'base': 'https://us-east1-ml-feeds.cloudfunctions.net/p...   \n",
       "3  {'type': 'text/plain', 'language': None, 'base': 'https://us-east1-ml-feeds.cloudfunctions.net/p...   \n",
       "4  {'type': 'text/plain', 'language': None, 'base': 'https://us-east1-ml-feeds.cloudfunctions.net/p...   \n",
       "\n",
       "                                                                                                 links  \\\n",
       "0  [{'rel': 'alternate', 'type': 'text/html', 'href': 'https://paperswithcode.com/paper/pose2mesh-g...   \n",
       "1  [{'rel': 'alternate', 'type': 'text/html', 'href': 'https://paperswithcode.com/paper/uncertainty...   \n",
       "2  [{'rel': 'alternate', 'type': 'text/html', 'href': 'https://paperswithcode.com/paper/yet-another...   \n",
       "3  [{'rel': 'alternate', 'type': 'text/html', 'href': 'https://paperswithcode.com/paper/lite-traini...   \n",
       "4  [{'rel': 'alternate', 'type': 'text/html', 'href': 'https://paperswithcode.com/paper/simultaneou...   \n",
       "\n",
       "                                                                            link  \\\n",
       "0  https://paperswithcode.com/paper/pose2mesh-graph-convolutional-network-for-3d   \n",
       "1       https://paperswithcode.com/paper/uncertainty-estimation-in-medical-image   \n",
       "2         https://paperswithcode.com/paper/yet-another-intermediate-level-attack   \n",
       "3       https://paperswithcode.com/paper/lite-training-strategies-for-portuguese   \n",
       "4      https://paperswithcode.com/paper/simultaneous-detection-and-tracking-with   \n",
       "\n",
       "                                                                                               summary  \\\n",
       "0  Most of the recent deep learning-based 3D human pose and mesh estimation methods regress the pos...   \n",
       "1  We use a randomly initialized convolutional network as parameterization of the reconstructed ima...   \n",
       "2  The transferability of adversarial examples across deep neural network (DNN) models is the crux ...   \n",
       "3  Despite the widespread adoption of deep learning for machine translation, it is still expensive ...   \n",
       "4  Deep learning-based Multiple Object Tracking (MOT) currently relies on off-the-shelf detectors f...   \n",
       "\n",
       "                                                                                        summary_detail  \\\n",
       "0  {'type': 'text/html', 'language': None, 'base': 'https://us-east1-ml-feeds.cloudfunctions.net/pw...   \n",
       "1  {'type': 'text/html', 'language': None, 'base': 'https://us-east1-ml-feeds.cloudfunctions.net/pw...   \n",
       "2  {'type': 'text/html', 'language': None, 'base': 'https://us-east1-ml-feeds.cloudfunctions.net/pw...   \n",
       "3  {'type': 'text/html', 'language': None, 'base': 'https://us-east1-ml-feeds.cloudfunctions.net/pw...   \n",
       "4  {'type': 'text/html', 'language': None, 'base': 'https://us-east1-ml-feeds.cloudfunctions.net/pw...   \n",
       "\n",
       "                                                                              id  \\\n",
       "0  https://paperswithcode.com/paper/pose2mesh-graph-convolutional-network-for-3d   \n",
       "1       https://paperswithcode.com/paper/uncertainty-estimation-in-medical-image   \n",
       "2         https://paperswithcode.com/paper/yet-another-intermediate-level-attack   \n",
       "3       https://paperswithcode.com/paper/lite-training-strategies-for-portuguese   \n",
       "4      https://paperswithcode.com/paper/simultaneous-detection-and-tracking-with   \n",
       "\n",
       "  guidislink  \\\n",
       "0      False   \n",
       "1      False   \n",
       "2      False   \n",
       "3      False   \n",
       "4      False   \n",
       "\n",
       "                                                                    tags  \\\n",
       "0                                                                    NaN   \n",
       "1           [{'term': 'Image denoising', 'scheme': None, 'label': None}]   \n",
       "2                                                                    NaN   \n",
       "3       [{'term': 'Machine translation', 'scheme': None, 'label': None}]   \n",
       "4  [{'term': 'Multiple object tracking', 'scheme': None, 'label': None}]   \n",
       "\n",
       "                                                                                                  text  \\\n",
       "0  Most of the recent deep learning-based 3D human pose and mesh estimation methods regress the pos...   \n",
       "1  We use a randomly initialized convolutional network as parameterization of the reconstructed ima...   \n",
       "2  The transferability of adversarial examples across deep neural network (DNN) models is the crux ...   \n",
       "3  Despite the widespread adoption of deep learning for machine translation, it is still expensive ...   \n",
       "4  Deep learning-based Multiple Object Tracking (MOT) currently relies on off-the-shelf detectors f...   \n",
       "\n",
       "   ... comments authors author author_detail updated updated_parsed content  \\\n",
       "0  ...      NaN     NaN    NaN           NaN     NaN            NaN     NaN   \n",
       "1  ...      NaN     NaN    NaN           NaN     NaN            NaN     NaN   \n",
       "2  ...      NaN     NaN    NaN           NaN     NaN            NaN     NaN   \n",
       "3  ...      NaN     NaN    NaN           NaN     NaN            NaN     NaN   \n",
       "4  ...      NaN     NaN    NaN           NaN     NaN            NaN     NaN   \n",
       "\n",
       "  href media_thumbnail        date  \n",
       "0  NaN             NaN  2020-08-25  \n",
       "1  NaN             NaN  2020-08-25  \n",
       "2  NaN             NaN  2020-08-25  \n",
       "3  NaN             NaN  2020-08-25  \n",
       "4  NaN             NaN  2020-08-25  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "class Searcher:\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name,\n",
    "        text_col,\n",
    "        use_gpu,\n",
    "        max_document_length=256,\n",
    "        quantize_model=True,\n",
    "        document_store_cls=database.memory.InMemoryDocumentStore\n",
    "    ):\n",
    "        self.text_col = text_col\n",
    "        self.embedding_col = text_col + '_emb'\n",
    "        self.max_document_length = max_document_length\n",
    "        self.model_name = model_name\n",
    "        self.document_store = document_store_cls(\n",
    "            embedding_field=self.embedding_col,\n",
    "        )\n",
    "        self.retriever = self._setup_retriever(use_gpu, quantize_model)\n",
    "\n",
    "    def _setup_retriever(self, use_gpu, quantize_model):\n",
    "        retriever = EmbeddingRetriever(\n",
    "            document_store=self.document_store,\n",
    "            embedding_model=self.model_name,\n",
    "            use_gpu=use_gpu)\n",
    "        if not use_gpu and quantize_model:\n",
    "            self.set_quantized_model(retriever)\n",
    "            \n",
    "        return retriever\n",
    "\n",
    "    def add_texts(\n",
    "        self,\n",
    "        df\n",
    "    ):\n",
    "        truncated_texts = [\n",
    "            ' '.join(tokenize.wordpunct_tokenize(text)[:self.max_document_length])\n",
    "            for text in df[self.text_col] \n",
    "        ]\n",
    "        article_embeddings = self.retriever.embed_queries(\n",
    "            texts=truncated_texts\n",
    "        )\n",
    "\n",
    "        df[self.embedding_col] = article_embeddings\n",
    "        self.document_store.write_documents(df.to_dict(orient='records'))\n",
    "    \n",
    "    def search(self, query, top_k=10, **kwargs):\n",
    "        if type(self.document_store) is database.elasticsearch.ElasticsearchDocumentStore:\n",
    "            querying_fn = self.document_store.query \n",
    "        elif type(self.document_store) is database.memory.InMemoryDocumentStore:\n",
    "            querying_fn = self.retriever.retrieve\n",
    "        return querying_fn(\n",
    "            query,\n",
    "            top_k=top_k,\n",
    "            **kwargs\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def set_quantized_model(cls, retriever):\n",
    "        quantized_model = torch.quantization.quantize_dynamic(\n",
    "            retriever.embedding_model.model,\n",
    "            {torch.nn.Linear}, dtype=torch.qint8\n",
    "        )\n",
    "        retriever.embedding_model.model = quantized_model\n",
    "        \n",
    "    @classmethod \n",
    "    def sigmoid(cls, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    @classmethod\n",
    "    def doc_to_dict(cls, doc):\n",
    "        d = {}\n",
    "        d['text'] = doc.text\n",
    "        d['title'] = doc.meta['title']\n",
    "        d['score'] = doc.query_score\n",
    "        d['link'] = doc.meta['link']\n",
    "        d['date'] = doc.meta['date']\n",
    "        d['feed'] = doc.meta['feed']\n",
    "        return d\n",
    "\n",
    "    def get_topic_score_df(self, raw_results, topic_strings, metadata_cols=['title', 'text', 'link', 'date']):\n",
    "        topic_query_strings = [\n",
    "            'text is about {}'.format(topic)\n",
    "            for topic in topic_strings\n",
    "        ]\n",
    "\n",
    "        results = [\n",
    "            self.doc_to_dict(doc)\n",
    "            for doc in raw_results \n",
    "        ]\n",
    "        result_embeddings = np.array([\n",
    "            doc.meta['text_emb']\n",
    "            for doc in raw_results\n",
    "        ]).astype('float32')\n",
    "        topic_query_embeddings = np.array(self.retriever.embed_passages(\n",
    "            list(topic_strings)\n",
    "        )).astype('float32')\n",
    "\n",
    "        scores_df = pd.DataFrame({})\n",
    "        scores_df['title'] = list(map(itemgetter('title'), results))\n",
    "        scores_df['text'] = list(map(itemgetter('text'), results))\n",
    "        scores_df['link'] = list(map(itemgetter('link'), results))\n",
    "        scores_df['date'] = list(map(itemgetter('date'), results))\n",
    "        scores_df['feed'] = list(map(itemgetter('feed'), results))\n",
    "\n",
    "        scores = pd.DataFrame(metrics.pairwise.cosine_similarity(\n",
    "            result_embeddings,\n",
    "            topic_query_embeddings\n",
    "        ))\n",
    "        scores.columns = topic_strings\n",
    "\n",
    "        scores_df = pd.concat(\n",
    "            [scores_df, self.sigmoid(scores)],\n",
    "            axis=1\n",
    "        )\n",
    "        return scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"deepset/sentence_bert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher = Searcher(\n",
    "    model_name,\n",
    "    'text',\n",
    "    use_gpu=use_gpu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 3/3 [00:07<00:00,  2.41s/ Batches]\n"
     ]
    }
   ],
   "source": [
    "searcher.add_texts(feed_df[['text', 'title', 'link', 'date', 'feed']].iloc[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_texts = feed_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_strings = pd.read_table('data/topics.txt', header=None).iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep learning\n",
      "natural language processing\n",
      "computer vision\n",
      "statistics\n",
      "implementation\n",
      "visualization\n",
      "industry\n",
      "software engineering\n",
      "reddit question\n",
      "arxiv\n",
      "cloud computing\n",
      "deployment\n",
      "competitions\n",
      "business\n",
      "business intelligence\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(topic_strings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_query_strings = [\n",
    "    'text is about {}'.format(topic)\n",
    "    for topic in topic_strings\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.53 Batches/s]\n"
     ]
    }
   ],
   "source": [
    "raw_results = searcher.search(\n",
    "    topic_query_strings[1],\n",
    "    top_k=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 4/4 [00:13<00:00,  3.30s/ Batches]\n"
     ]
    }
   ],
   "source": [
    "scores_df = searcher.get_topic_score_df(raw_results, topic_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow0_col5 {\n",
       "            background-color:  #239323;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow0_col6 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow0_col7 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow0_col8 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow0_col9 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow0_col10 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow0_col11 {\n",
       "            background-color:  #c4edc4;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow0_col12 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow0_col13 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow0_col14 {\n",
       "            background-color:  #7ec67e;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow0_col15 {\n",
       "            background-color:  #daf9da;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow0_col16 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow0_col17 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow0_col18 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow0_col19 {\n",
       "            background-color:  #2f9a2f;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow1_col5 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow1_col6 {\n",
       "            background-color:  #99d599;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow1_col7 {\n",
       "            background-color:  #64b764;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow1_col8 {\n",
       "            background-color:  #5bb25b;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow1_col9 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow1_col10 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow1_col11 {\n",
       "            background-color:  #a0d9a0;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow1_col12 {\n",
       "            background-color:  #79c379;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow1_col13 {\n",
       "            background-color:  #1f911f;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow1_col14 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow1_col15 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow1_col16 {\n",
       "            background-color:  #319b31;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow1_col17 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow1_col18 {\n",
       "            background-color:  #a7dda7;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow1_col19 {\n",
       "            background-color:  #a5dba5;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow2_col5 {\n",
       "            background-color:  #c2ecc2;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow2_col6 {\n",
       "            background-color:  #229322;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow2_col7 {\n",
       "            background-color:  #168c16;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow2_col8 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow2_col9 {\n",
       "            background-color:  #289628;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow2_col10 {\n",
       "            background-color:  #0e870e;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow2_col11 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow2_col12 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow2_col13 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow2_col14 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow2_col15 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow2_col16 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow2_col17 {\n",
       "            background-color:  #2e992e;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow2_col18 {\n",
       "            background-color:  #45a645;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow2_col19 {\n",
       "            background-color:  #178d17;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow3_col5 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow3_col6 {\n",
       "            background-color:  #6dbc6d;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow3_col7 {\n",
       "            background-color:  #bde9bd;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow3_col8 {\n",
       "            background-color:  #62b662;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow3_col9 {\n",
       "            background-color:  #6dbc6d;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow3_col10 {\n",
       "            background-color:  #3fa33f;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow3_col11 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow3_col12 {\n",
       "            background-color:  #178c17;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow3_col13 {\n",
       "            background-color:  #48a848;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow3_col14 {\n",
       "            background-color:  #d8f8d8;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow3_col15 {\n",
       "            background-color:  #94d294;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow3_col16 {\n",
       "            background-color:  #7ac37a;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow3_col17 {\n",
       "            background-color:  #ddfbdd;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow3_col18 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow3_col19 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow4_col5 {\n",
       "            background-color:  #299729;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow4_col6 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow4_col7 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow4_col8 {\n",
       "            background-color:  #018001;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow4_col9 {\n",
       "            background-color:  #5db35d;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow4_col10 {\n",
       "            background-color:  #4dab4d;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow4_col11 {\n",
       "            background-color:  #ade0ad;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow4_col12 {\n",
       "            background-color:  #078407;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow4_col13 {\n",
       "            background-color:  #2a972a;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow4_col14 {\n",
       "            background-color:  #75c175;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow4_col15 {\n",
       "            background-color:  #6dbc6d;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow4_col16 {\n",
       "            background-color:  #a7dda7;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow4_col17 {\n",
       "            background-color:  #058305;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow4_col18 {\n",
       "            background-color:  #6dbc6d;\n",
       "            color:  #000000;\n",
       "        }    #T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow4_col19 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }</style><table id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43a\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >title</th>        <th class=\"col_heading level0 col1\" >text</th>        <th class=\"col_heading level0 col2\" >link</th>        <th class=\"col_heading level0 col3\" >date</th>        <th class=\"col_heading level0 col4\" >feed</th>        <th class=\"col_heading level0 col5\" >deep learning</th>        <th class=\"col_heading level0 col6\" >natural language processing</th>        <th class=\"col_heading level0 col7\" >computer vision</th>        <th class=\"col_heading level0 col8\" >statistics</th>        <th class=\"col_heading level0 col9\" >implementation</th>        <th class=\"col_heading level0 col10\" >visualization</th>        <th class=\"col_heading level0 col11\" >industry</th>        <th class=\"col_heading level0 col12\" >software engineering</th>        <th class=\"col_heading level0 col13\" >reddit question</th>        <th class=\"col_heading level0 col14\" >arxiv</th>        <th class=\"col_heading level0 col15\" >cloud computing</th>        <th class=\"col_heading level0 col16\" >deployment</th>        <th class=\"col_heading level0 col17\" >competitions</th>        <th class=\"col_heading level0 col18\" >business</th>        <th class=\"col_heading level0 col19\" >business intelligence</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43alevel0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow0_col0\" class=\"data row0 col0\" >Lite Training Strategies for Portuguese-English and English-Portuguese Translation</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow0_col1\" class=\"data row0 col1\" >Despite the widespread adoption of deep learning for machine translation, it is still expensive to develop high-quality translation models. Code: https://github.com/unicamp-dl/Lite-T5-Translation</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow0_col2\" class=\"data row0 col2\" >https://paperswithcode.com/paper/lite-training-strategies-for-portuguese</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow0_col3\" class=\"data row0 col3\" >2020-08-25</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow0_col4\" class=\"data row0 col4\" >https://us-east1-ml-feeds.cloudfunctions.net/pwc/latest</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow0_col5\" class=\"data row0 col5\" >0.591616</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow0_col6\" class=\"data row0 col6\" >0.579322</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow0_col7\" class=\"data row0 col7\" >0.550228</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow0_col8\" class=\"data row0 col8\" >0.513698</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow0_col9\" class=\"data row0 col9\" >0.499190</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow0_col10\" class=\"data row0 col10\" >0.523212</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow0_col11\" class=\"data row0 col11\" >0.524268</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow0_col12\" class=\"data row0 col12\" >0.568552</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow0_col13\" class=\"data row0 col13\" >0.513954</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow0_col14\" class=\"data row0 col14\" >0.504317</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow0_col15\" class=\"data row0 col15\" >0.518718</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow0_col16\" class=\"data row0 col16\" >0.490565</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow0_col17\" class=\"data row0 col17\" >0.516409</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow0_col18\" class=\"data row0 col18\" >0.513655</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow0_col19\" class=\"data row0 col19\" >0.531931</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43alevel0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow1_col0\" class=\"data row1 col0\" >MATNet: Motion-Attentive Transition Network for Zero-Shot Video Object Segmentation</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow1_col1\" class=\"data row1 col1\" >To further demonstrate the generalization ability of our spatiotemporal learning framework, we extend MATNet to another relevant task: dynamic visual attention prediction (DVAP). Code: https://github.com/tfzhou/MATNet</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow1_col2\" class=\"data row1 col2\" >https://paperswithcode.com/paper/matnet-motion-attentive-transition-network</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow1_col3\" class=\"data row1 col3\" >2020-08-25</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow1_col4\" class=\"data row1 col4\" >https://us-east1-ml-feeds.cloudfunctions.net/pwc/latest</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow1_col5\" class=\"data row1 col5\" >0.594966</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow1_col6\" class=\"data row1 col6\" >0.567715</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow1_col7\" class=\"data row1 col7\" >0.562802</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow1_col8\" class=\"data row1 col8\" >0.526444</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow1_col9\" class=\"data row1 col9\" >0.577816</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow1_col10\" class=\"data row1 col10\" >0.588119</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow1_col11\" class=\"data row1 col11\" >0.529175</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow1_col12\" class=\"data row1 col12\" >0.579824</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow1_col13\" class=\"data row1 col13\" >0.548776</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow1_col14\" class=\"data row1 col14\" >0.492425</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow1_col15\" class=\"data row1 col15\" >0.516866</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow1_col16\" class=\"data row1 col16\" >0.527176</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow1_col17\" class=\"data row1 col17\" >0.557264</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow1_col18\" class=\"data row1 col18\" >0.498854</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow1_col19\" class=\"data row1 col19\" >0.509820</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43alevel0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow2_col0\" class=\"data row2 col0\" >Every Pixel Matters: Center-aware Feature Alignment for Domain Adaptive Object Detector</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow2_col1\" class=\"data row2 col1\" >A domain adaptive object detector aims to adapt itself to unseen domains that may contain variations of object appearance, viewpoints or backgrounds. Code: https://github.com/chengchunhsu/EveryPixelMatters</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow2_col2\" class=\"data row2 col2\" >https://paperswithcode.com/paper/every-pixel-matters-center-aware-feature</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow2_col3\" class=\"data row2 col3\" >2020-08-25</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow2_col4\" class=\"data row2 col4\" >https://us-east1-ml-feeds.cloudfunctions.net/pwc/latest</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow2_col5\" class=\"data row2 col5\" >0.576378</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow2_col6\" class=\"data row2 col6\" >0.576696</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow2_col7\" class=\"data row2 col7\" >0.570305</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow2_col8\" class=\"data row2 col8\" >0.534881</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow2_col9\" class=\"data row2 col9\" >0.564288</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow2_col10\" class=\"data row2 col10\" >0.584288</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow2_col11\" class=\"data row2 col11\" >0.551016</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow2_col12\" class=\"data row2 col12\" >0.592284</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow2_col13\" class=\"data row2 col13\" >0.554121</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow2_col14\" class=\"data row2 col14\" >0.518748</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow2_col15\" class=\"data row2 col15\" >0.552403</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow2_col16\" class=\"data row2 col16\" >0.537362</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow2_col17\" class=\"data row2 col17\" >0.549096</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow2_col18\" class=\"data row2 col18\" >0.507504</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow2_col19\" class=\"data row2 col19\" >0.536153</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43alevel0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow3_col0\" class=\"data row3 col0\" >Pose2Mesh: Graph Convolutional Network for 3D Human Pose and Mesh Recovery from a 2D Human Pose</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow3_col1\" class=\"data row3 col1\" >Most of the recent deep learning-based 3D human pose and mesh estimation methods regress the pose and shape parameters of human mesh models, such as SMPL and MANO, from an input image. Code: https://github.com/hongsukchoi/Pose2Mesh_RELEASE</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow3_col2\" class=\"data row3 col2\" >https://paperswithcode.com/paper/pose2mesh-graph-convolutional-network-for-3d</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow3_col3\" class=\"data row3 col3\" >2020-08-25</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow3_col4\" class=\"data row3 col4\" >https://us-east1-ml-feeds.cloudfunctions.net/pwc/latest</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow3_col5\" class=\"data row3 col5\" >0.573001</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow3_col6\" class=\"data row3 col6\" >0.571049</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow3_col7\" class=\"data row3 col7\" >0.554202</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow3_col8\" class=\"data row3 col8\" >0.525803</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow3_col9\" class=\"data row3 col9\" >0.540447</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow3_col10\" class=\"data row3 col10\" >0.570229</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow3_col11\" class=\"data row3 col11\" >0.519744</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow3_col12\" class=\"data row3 col12\" >0.589925</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow3_col13\" class=\"data row3 col13\" >0.541486</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow3_col14\" class=\"data row3 col14\" >0.494045</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow3_col15\" class=\"data row3 col15\" >0.529494</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow3_col16\" class=\"data row3 col16\" >0.512597</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow3_col17\" class=\"data row3 col17\" >0.517936</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow3_col18\" class=\"data row3 col18\" >0.493392</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow3_col19\" class=\"data row3 col19\" >0.497597</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43alevel0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow4_col0\" class=\"data row4 col0\" >Simultaneous Detection and Tracking with Motion Modelling for Multiple Object Tracking</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow4_col1\" class=\"data row4 col1\" >Deep learning-based Multiple Object Tracking (MOT) currently relies on off-the-shelf detectors for tracking-by-detection. This results in deep models that are detector biased and evaluations that are detector influenced. Code: https://github.com/shijieS/DMMN</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow4_col2\" class=\"data row4 col2\" >https://paperswithcode.com/paper/simultaneous-detection-and-tracking-with</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow4_col3\" class=\"data row4 col3\" >2020-08-25</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow4_col4\" class=\"data row4 col4\" >https://us-east1-ml-feeds.cloudfunctions.net/pwc/latest</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow4_col5\" class=\"data row4 col5\" >0.590944</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow4_col6\" class=\"data row4 col6\" >0.561941</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow4_col7\" class=\"data row4 col7\" >0.572436</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow4_col8\" class=\"data row4 col8\" >0.534769</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow4_col9\" class=\"data row4 col9\" >0.546152</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow4_col10\" class=\"data row4 col10\" >0.566145</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow4_col11\" class=\"data row4 col11\" >0.527532</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow4_col12\" class=\"data row4 col12\" >0.591534</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow4_col13\" class=\"data row4 col13\" >0.546668</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow4_col14\" class=\"data row4 col14\" >0.505290</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow4_col15\" class=\"data row4 col15\" >0.535472</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow4_col16\" class=\"data row4 col16\" >0.503323</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow4_col17\" class=\"data row4 col17\" >0.556196</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow4_col18\" class=\"data row4 col18\" >0.504045</td>\n",
       "                        <td id=\"T_53a4d146_e6e5_11ea_bf85_9df1ab2be43arow4_col19\" class=\"data row4 col19\" >0.540687</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fa0f680e640>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df.head().style.background_gradient(cmap=cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-eda4e5adc56c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfeed_topics_statistics_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'feed'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_topics_statistics_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'RdBu_r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplpot\n",
    "\n",
    "feed_topics_statistics_df = scores_df.groupby('feed').agg('mean')\n",
    "plt.figure(figsize=(16, 10))\n",
    "sns.heatmap(feed_topics_statistics_df, annot=True, cmap='RdBu_r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
