{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp haystack_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:09<00:00,  1.60it/s]\n",
      "/home/piotr/Documents/pytorch_hackathon/pytorch_hackathon/rss_feeds.py:64: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 64 of the file /home/piotr/Documents/pytorch_hackathon/pytorch_hackathon/rss_feeds.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  feed_df['text'] = feed_df['summary'].apply(lambda s: bs4.BeautifulSoup(s).text)\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "from nltk import tokenize\n",
    "from operator import itemgetter\n",
    "\n",
    "from haystack.database.elasticsearch import ElasticsearchDocumentStore\n",
    "from haystack.database.memory import InMemoryDocumentStore\n",
    "\n",
    "from haystack.retriever.dense import EmbeddingRetriever\n",
    "from pytorch_hackathon import rss_feeds\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = sns.light_palette(\"green\", as_cmap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/piotr/Documents/pytorch_hackathon\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feeds.txt  topics.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rss_feed_urls = list(pd.read_table('data/feeds.txt', header=None).iloc[:,0].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:10<00:00,  1.55it/s]\n"
     ]
    }
   ],
   "source": [
    "feed_df = rss_feeds.get_feed_df(rss_feed_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print = pprint.PrettyPrinter(indent=2).pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>title_detail</th>\n",
       "      <th>links</th>\n",
       "      <th>link</th>\n",
       "      <th>summary</th>\n",
       "      <th>summary_detail</th>\n",
       "      <th>id</th>\n",
       "      <th>guidislink</th>\n",
       "      <th>tags</th>\n",
       "      <th>text</th>\n",
       "      <th>...</th>\n",
       "      <th>comments</th>\n",
       "      <th>authors</th>\n",
       "      <th>author</th>\n",
       "      <th>author_detail</th>\n",
       "      <th>updated</th>\n",
       "      <th>updated_parsed</th>\n",
       "      <th>content</th>\n",
       "      <th>href</th>\n",
       "      <th>media_thumbnail</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PARADE: Passage Representation Aggregation for Document Reranking</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base': 'https://us-east1-ml-feeds.cloudfunctions.net/p...</td>\n",
       "      <td>[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://paperswithcode.com/paper/parade-pass...</td>\n",
       "      <td>https://paperswithcode.com/paper/parade-passage-representation-aggregation-for</td>\n",
       "      <td>We present PARADE, an end-to-end Transformer-based model that considers document-level context f...</td>\n",
       "      <td>{'type': 'text/html', 'language': None, 'base': 'https://us-east1-ml-feeds.cloudfunctions.net/pw...</td>\n",
       "      <td>https://paperswithcode.com/paper/parade-passage-representation-aggregation-for</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We present PARADE, an end-to-end Transformer-based model that considers document-level context f...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-08-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MATNet: Motion-Attentive Transition Network for Zero-Shot Video Object Segmentation</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base': 'https://us-east1-ml-feeds.cloudfunctions.net/p...</td>\n",
       "      <td>[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://paperswithcode.com/paper/matnet-moti...</td>\n",
       "      <td>https://paperswithcode.com/paper/matnet-motion-attentive-transition-network</td>\n",
       "      <td>To further demonstrate the generalization ability of our spatiotemporal learning framework, we e...</td>\n",
       "      <td>{'type': 'text/html', 'language': None, 'base': 'https://us-east1-ml-feeds.cloudfunctions.net/pw...</td>\n",
       "      <td>https://paperswithcode.com/paper/matnet-motion-attentive-transition-network</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'term': 'Semantic segmentation', 'scheme': None, 'label': None}, {'term': 'Unsupervised video ...</td>\n",
       "      <td>To further demonstrate the generalization ability of our spatiotemporal learning framework, we e...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-08-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yet Another Intermediate-Level Attack</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base': 'https://us-east1-ml-feeds.cloudfunctions.net/p...</td>\n",
       "      <td>[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://paperswithcode.com/paper/yet-another...</td>\n",
       "      <td>https://paperswithcode.com/paper/yet-another-intermediate-level-attack</td>\n",
       "      <td>The transferability of adversarial examples across deep neural network (DNN) models is the crux ...</td>\n",
       "      <td>{'type': 'text/html', 'language': None, 'base': 'https://us-east1-ml-feeds.cloudfunctions.net/pw...</td>\n",
       "      <td>https://paperswithcode.com/paper/yet-another-intermediate-level-attack</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The transferability of adversarial examples across deep neural network (DNN) models is the crux ...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-08-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Uncertainty Estimation in Medical Image Denoising with Bayesian Deep Image Prior</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base': 'https://us-east1-ml-feeds.cloudfunctions.net/p...</td>\n",
       "      <td>[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://paperswithcode.com/paper/uncertainty...</td>\n",
       "      <td>https://paperswithcode.com/paper/uncertainty-estimation-in-medical-image</td>\n",
       "      <td>We use a randomly initialized convolutional network as parameterization of the reconstructed ima...</td>\n",
       "      <td>{'type': 'text/html', 'language': None, 'base': 'https://us-east1-ml-feeds.cloudfunctions.net/pw...</td>\n",
       "      <td>https://paperswithcode.com/paper/uncertainty-estimation-in-medical-image</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'term': 'Image denoising', 'scheme': None, 'label': None}]</td>\n",
       "      <td>We use a randomly initialized convolutional network as parameterization of the reconstructed ima...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-08-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Simultaneous Detection and Tracking with Motion Modelling for Multiple Object Tracking</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base': 'https://us-east1-ml-feeds.cloudfunctions.net/p...</td>\n",
       "      <td>[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://paperswithcode.com/paper/simultaneou...</td>\n",
       "      <td>https://paperswithcode.com/paper/simultaneous-detection-and-tracking-with</td>\n",
       "      <td>Deep learning-based Multiple Object Tracking (MOT) currently relies on off-the-shelf detectors f...</td>\n",
       "      <td>{'type': 'text/html', 'language': None, 'base': 'https://us-east1-ml-feeds.cloudfunctions.net/pw...</td>\n",
       "      <td>https://paperswithcode.com/paper/simultaneous-detection-and-tracking-with</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'term': 'Multiple object tracking', 'scheme': None, 'label': None}]</td>\n",
       "      <td>Deep learning-based Multiple Object Tracking (MOT) currently relies on off-the-shelf detectors f...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-08-23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                    title  \\\n",
       "0                       PARADE: Passage Representation Aggregation for Document Reranking   \n",
       "1     MATNet: Motion-Attentive Transition Network for Zero-Shot Video Object Segmentation   \n",
       "2                                                   Yet Another Intermediate-Level Attack   \n",
       "3        Uncertainty Estimation in Medical Image Denoising with Bayesian Deep Image Prior   \n",
       "4  Simultaneous Detection and Tracking with Motion Modelling for Multiple Object Tracking   \n",
       "\n",
       "                                                                                          title_detail  \\\n",
       "0  {'type': 'text/plain', 'language': None, 'base': 'https://us-east1-ml-feeds.cloudfunctions.net/p...   \n",
       "1  {'type': 'text/plain', 'language': None, 'base': 'https://us-east1-ml-feeds.cloudfunctions.net/p...   \n",
       "2  {'type': 'text/plain', 'language': None, 'base': 'https://us-east1-ml-feeds.cloudfunctions.net/p...   \n",
       "3  {'type': 'text/plain', 'language': None, 'base': 'https://us-east1-ml-feeds.cloudfunctions.net/p...   \n",
       "4  {'type': 'text/plain', 'language': None, 'base': 'https://us-east1-ml-feeds.cloudfunctions.net/p...   \n",
       "\n",
       "                                                                                                 links  \\\n",
       "0  [{'rel': 'alternate', 'type': 'text/html', 'href': 'https://paperswithcode.com/paper/parade-pass...   \n",
       "1  [{'rel': 'alternate', 'type': 'text/html', 'href': 'https://paperswithcode.com/paper/matnet-moti...   \n",
       "2  [{'rel': 'alternate', 'type': 'text/html', 'href': 'https://paperswithcode.com/paper/yet-another...   \n",
       "3  [{'rel': 'alternate', 'type': 'text/html', 'href': 'https://paperswithcode.com/paper/uncertainty...   \n",
       "4  [{'rel': 'alternate', 'type': 'text/html', 'href': 'https://paperswithcode.com/paper/simultaneou...   \n",
       "\n",
       "                                                                             link  \\\n",
       "0  https://paperswithcode.com/paper/parade-passage-representation-aggregation-for   \n",
       "1     https://paperswithcode.com/paper/matnet-motion-attentive-transition-network   \n",
       "2          https://paperswithcode.com/paper/yet-another-intermediate-level-attack   \n",
       "3        https://paperswithcode.com/paper/uncertainty-estimation-in-medical-image   \n",
       "4       https://paperswithcode.com/paper/simultaneous-detection-and-tracking-with   \n",
       "\n",
       "                                                                                               summary  \\\n",
       "0  We present PARADE, an end-to-end Transformer-based model that considers document-level context f...   \n",
       "1  To further demonstrate the generalization ability of our spatiotemporal learning framework, we e...   \n",
       "2  The transferability of adversarial examples across deep neural network (DNN) models is the crux ...   \n",
       "3  We use a randomly initialized convolutional network as parameterization of the reconstructed ima...   \n",
       "4  Deep learning-based Multiple Object Tracking (MOT) currently relies on off-the-shelf detectors f...   \n",
       "\n",
       "                                                                                        summary_detail  \\\n",
       "0  {'type': 'text/html', 'language': None, 'base': 'https://us-east1-ml-feeds.cloudfunctions.net/pw...   \n",
       "1  {'type': 'text/html', 'language': None, 'base': 'https://us-east1-ml-feeds.cloudfunctions.net/pw...   \n",
       "2  {'type': 'text/html', 'language': None, 'base': 'https://us-east1-ml-feeds.cloudfunctions.net/pw...   \n",
       "3  {'type': 'text/html', 'language': None, 'base': 'https://us-east1-ml-feeds.cloudfunctions.net/pw...   \n",
       "4  {'type': 'text/html', 'language': None, 'base': 'https://us-east1-ml-feeds.cloudfunctions.net/pw...   \n",
       "\n",
       "                                                                               id  \\\n",
       "0  https://paperswithcode.com/paper/parade-passage-representation-aggregation-for   \n",
       "1     https://paperswithcode.com/paper/matnet-motion-attentive-transition-network   \n",
       "2          https://paperswithcode.com/paper/yet-another-intermediate-level-attack   \n",
       "3        https://paperswithcode.com/paper/uncertainty-estimation-in-medical-image   \n",
       "4       https://paperswithcode.com/paper/simultaneous-detection-and-tracking-with   \n",
       "\n",
       "  guidislink  \\\n",
       "0      False   \n",
       "1      False   \n",
       "2      False   \n",
       "3      False   \n",
       "4      False   \n",
       "\n",
       "                                                                                                  tags  \\\n",
       "0                                                                                                  NaN   \n",
       "1  [{'term': 'Semantic segmentation', 'scheme': None, 'label': None}, {'term': 'Unsupervised video ...   \n",
       "2                                                                                                  NaN   \n",
       "3                                         [{'term': 'Image denoising', 'scheme': None, 'label': None}]   \n",
       "4                                [{'term': 'Multiple object tracking', 'scheme': None, 'label': None}]   \n",
       "\n",
       "                                                                                                  text  \\\n",
       "0  We present PARADE, an end-to-end Transformer-based model that considers document-level context f...   \n",
       "1  To further demonstrate the generalization ability of our spatiotemporal learning framework, we e...   \n",
       "2  The transferability of adversarial examples across deep neural network (DNN) models is the crux ...   \n",
       "3  We use a randomly initialized convolutional network as parameterization of the reconstructed ima...   \n",
       "4  Deep learning-based Multiple Object Tracking (MOT) currently relies on off-the-shelf detectors f...   \n",
       "\n",
       "   ... comments authors author author_detail updated updated_parsed content  \\\n",
       "0  ...      NaN     NaN    NaN           NaN     NaN            NaN     NaN   \n",
       "1  ...      NaN     NaN    NaN           NaN     NaN            NaN     NaN   \n",
       "2  ...      NaN     NaN    NaN           NaN     NaN            NaN     NaN   \n",
       "3  ...      NaN     NaN    NaN           NaN     NaN            NaN     NaN   \n",
       "4  ...      NaN     NaN    NaN           NaN     NaN            NaN     NaN   \n",
       "\n",
       "  href media_thumbnail        date  \n",
       "0  NaN             NaN  2020-08-23  \n",
       "1  NaN             NaN  2020-08-23  \n",
       "2  NaN             NaN  2020-08-23  \n",
       "3  NaN             NaN  2020-08-23  \n",
       "4  NaN             NaN  2020-08-23  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "\n",
    "class Searcher:\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name,\n",
    "        text_col,\n",
    "        use_gpu,\n",
    "        max_document_length=256,\n",
    "        quantize_model=True,\n",
    "        document_store_cls=InMemoryDocumentStore\n",
    "    ):\n",
    "        self.text_col = text_col\n",
    "        self.embedding_col = text_col + '_emb'\n",
    "        self.max_document_length = max_document_length\n",
    "        self.model_name = model_name\n",
    "        self.document_store = document_store_cls(\n",
    "            embedding_field=self.embedding_col,\n",
    "        )\n",
    "        self.retriever = self._setup_retriever(use_gpu, quantize_model)\n",
    "\n",
    "    def _setup_retriever(self, use_gpu, quantize_model):\n",
    "        retriever = EmbeddingRetriever(\n",
    "            document_store=self.document_store,\n",
    "            embedding_model=self.model_name,\n",
    "            use_gpu=use_gpu)\n",
    "        if not use_gpu and quantize_model:\n",
    "            self.set_quantized_model(retriever)\n",
    "            \n",
    "        return retriever\n",
    "\n",
    "    def add_texts(\n",
    "        self,\n",
    "        df\n",
    "    ):\n",
    "        truncated_texts = [\n",
    "            ' '.join(tokenize.wordpunct_tokenize(text)[:self.max_document_length])\n",
    "            for text in df[self.text_col] \n",
    "        ]\n",
    "        article_embeddings = self.retriever.embed_queries(\n",
    "            texts=truncated_texts\n",
    "        )\n",
    "\n",
    "        df[self.embedding_col] = article_embeddings\n",
    "        self.document_store.write_documents(df.to_dict(orient='records'))\n",
    "    \n",
    "    @classmethod\n",
    "    def set_quantized_model(cls, retriever):\n",
    "        quantized_model = torch.quantization.quantize_dynamic(\n",
    "            retriever.embedding_model.model,\n",
    "            {torch.nn.Linear}, dtype=torch.qint8\n",
    "        )\n",
    "        retriever.embedding_model.model = quantized_model\n",
    "        \n",
    "    @classmethod \n",
    "    def sigmoid(cls, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    @classmethod\n",
    "    def doc_to_dict(cls, doc):\n",
    "        d = {}\n",
    "        d['text'] = doc.text\n",
    "        d['title'] = doc.meta['title']\n",
    "        d['score'] = doc.query_score\n",
    "        d['link'] = doc.meta['link']\n",
    "        d['date'] = doc.meta['date']\n",
    "        return d\n",
    "\n",
    "    def get_topic_score_df(self, raw_results, topic_strings):\n",
    "        topic_query_strings = [\n",
    "            'text is about {}'.format(topic)\n",
    "            for topic in topic_strings\n",
    "        ]\n",
    "\n",
    "        results = [\n",
    "            self.doc_to_dict(doc)\n",
    "            for doc in raw_results \n",
    "        ]\n",
    "        result_embeddings = np.array([\n",
    "            doc.meta['text_emb']\n",
    "            for doc in raw_results\n",
    "        ]).astype('float32')\n",
    "        topic_query_embeddings = np.array(self.retriever.embed_passages(\n",
    "            list(topic_strings)\n",
    "        )).astype('float32')\n",
    "\n",
    "        scores_df = pd.DataFrame({})\n",
    "        scores_df['title'] = list(map(itemgetter('title'), results))\n",
    "        scores_df['text'] = list(map(itemgetter('text'), results))\n",
    "        scores_df['link'] = list(map(itemgetter('link'), results))\n",
    "        scores_df['date'] = list(map(itemgetter('date'), results))\n",
    "\n",
    "        scores = pd.DataFrame(metrics.pairwise.cosine_similarity(\n",
    "            result_embeddings,\n",
    "            topic_query_embeddings\n",
    "        ))\n",
    "        scores.columns = topic_strings\n",
    "\n",
    "        scores_df = pd.concat(\n",
    "            [scores_df, self.sigmoid(scores)],\n",
    "            axis=1\n",
    "        )\n",
    "        return scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"deepset/sentence_bert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/17/2020 17:55:34 - INFO - haystack.retriever.dense -   Init retriever using embeddings of model deepset/sentence_bert\n",
      "08/17/2020 17:55:34 - INFO - farm.utils -   device: cuda n_gpu: 1, distributed training: False, automatic mixed precision training: None\n",
      "08/17/2020 17:55:34 - INFO - farm.infer -   Could not find `deepset/sentence_bert` locally. Try to download from model hub ...\n",
      "08/17/2020 17:55:34 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at /home/kuba/.cache/torch/transformers/fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n",
      "08/17/2020 17:55:36 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "08/17/2020 17:55:36 - INFO - transformers.modeling_utils -   All the weights of BertModel were initialized from the model checkpoint at deepset/sentence_bert.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "08/17/2020 17:55:36 - WARNING - farm.modeling.language_model -   Could not automatically detect from language model name what language it is. \n",
      "\t We guess it's an *ENGLISH* model ... \n",
      "\t If not: Init the language model by supplying the 'language' param.\n",
      "08/17/2020 17:55:39 - INFO - farm.modeling.tokenization -   Loading tokenizer of type 'BertTokenizer'\n",
      "08/17/2020 17:55:39 - INFO - transformers.tokenization_utils_base -   Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "08/17/2020 17:55:41 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at /home/kuba/.cache/torch/transformers/205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "08/17/2020 17:55:41 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at /home/kuba/.cache/torch/transformers/989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "08/17/2020 17:55:41 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at /home/kuba/.cache/torch/transformers/07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "08/17/2020 17:55:41 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "08/17/2020 17:55:41 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer.json from cache at None\n",
      "08/17/2020 17:55:41 - INFO - farm.utils -   device: cuda n_gpu: 1, distributed training: False, automatic mixed precision training: None\n"
     ]
    }
   ],
   "source": [
    "searcher = Searcher(\n",
    "    model_name,\n",
    "    'text',\n",
    "    use_gpu=use_gpu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/17/2020 17:55:42 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
      "08/17/2020 17:55:42 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 57-0\n",
      "Clear Text: \n",
      " \ttext: Comments\n",
      "Tokenized: \n",
      " \ttokens: ['comments']\n",
      " \toffsets: [0]\n",
      " \tstart_of_word: [True]\n",
      "Features: \n",
      " \tinput_ids: [101, 7928, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tpadding_mask: [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "_____________________________________________________\n",
      "08/17/2020 17:55:42 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 140-0\n",
      "Clear Text: \n",
      " \ttext: Rapid progress in machine learning has led to an exponential growth in the number of machine learning papers , with a new paper published … Continue reading on PapersWithCode »\n",
      "Tokenized: \n",
      " \ttokens: ['rapid', 'progress', 'in', 'machine', 'learning', 'has', 'led', 'to', 'an', 'exponential', 'growth', 'in', 'the', 'number', 'of', 'machine', 'learning', 'papers', ',', 'with', 'a', 'new', 'paper', 'published', '…', 'continue', 'reading', 'on', 'papers', '##with', '##code', '»']\n",
      " \toffsets: [0, 6, 15, 18, 26, 35, 39, 43, 46, 49, 61, 68, 71, 75, 82, 85, 93, 102, 109, 111, 116, 118, 122, 128, 138, 140, 149, 157, 160, 166, 170, 175]\n",
      " \tstart_of_word: [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, True]\n",
      "Features: \n",
      " \tinput_ids: [101, 5915, 5082, 1999, 3698, 4083, 2038, 2419, 2000, 2019, 27258, 3930, 1999, 1996, 2193, 1997, 3698, 4083, 4981, 1010, 2007, 1037, 2047, 3259, 2405, 1529, 3613, 3752, 2006, 4981, 24415, 16044, 1090, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "_____________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 74/74 [00:10<00:00,  7.10 Batches/s]\n"
     ]
    }
   ],
   "source": [
    "searcher.add_texts(feed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_texts = feed_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_strings = pd.read_table('data/topics.txt', header=None).iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep learning\n",
      "natural language processing\n",
      "computer vision\n",
      "statistics\n",
      "implementation\n",
      "visualization\n",
      "industry\n",
      "software engineering\n",
      "reddit question\n",
      "arxiv\n",
      "cloud computing\n",
      "deployment\n",
      "competitions\n",
      "business\n",
      "business intelligence\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(topic_strings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_query_strings = [\n",
    "    'text is about {}'.format(topic)\n",
    "    for topic in topic_strings\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/17/2020 17:55:53 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
      "08/17/2020 17:55:53 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 0-0\n",
      "Clear Text: \n",
      " \ttext: text is about natural language processing\n",
      "Tokenized: \n",
      " \ttokens: ['text', 'is', 'about', 'natural', 'language', 'processing']\n",
      " \toffsets: [0, 5, 8, 14, 22, 31]\n",
      " \tstart_of_word: [True, True, True, True, True, True]\n",
      "Features: \n",
      " \tinput_ids: [101, 3793, 2003, 2055, 3019, 2653, 6364, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "_____________________________________________________\n",
      "08/17/2020 17:55:53 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 0-0\n",
      "Clear Text: \n",
      " \ttext: text is about natural language processing\n",
      "Tokenized: \n",
      " \ttokens: ['text', 'is', 'about', 'natural', 'language', 'processing']\n",
      " \toffsets: [0, 5, 8, 14, 22, 31]\n",
      " \tstart_of_word: [True, True, True, True, True, True]\n",
      "Features: \n",
      " \tinput_ids: [101, 3793, 2003, 2055, 3019, 2653, 6364, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "_____________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00, 33.47 Batches/s]\n"
     ]
    }
   ],
   "source": [
    "raw_results = searcher.retriever.retrieve(\n",
    "    topic_query_strings[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/17/2020 17:55:53 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
      "08/17/2020 17:55:53 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 9-0\n",
      "Clear Text: \n",
      " \ttext: arxiv\n",
      "Tokenized: \n",
      " \ttokens: ['ar', '##xi', '##v']\n",
      " \toffsets: [0, 2, 4]\n",
      " \tstart_of_word: [True, False, False]\n",
      "Features: \n",
      " \tinput_ids: [101, 12098, 9048, 2615, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "_____________________________________________________\n",
      "08/17/2020 17:55:53 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 0-0\n",
      "Clear Text: \n",
      " \ttext: deep learning\n",
      "Tokenized: \n",
      " \ttokens: ['deep', 'learning']\n",
      " \toffsets: [0, 5]\n",
      " \tstart_of_word: [True, True]\n",
      "Features: \n",
      " \tinput_ids: [101, 2784, 4083, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tpadding_mask: [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "_____________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 4/4 [00:00<00:00,  7.71 Batches/s]\n",
      "08/17/2020 17:55:53 - INFO - numexpr.utils -   Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "08/17/2020 17:55:53 - INFO - numexpr.utils -   NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "scores_df = searcher.get_topic_score_df( raw_results, topic_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row0_col3 {\n",
       "            background-color:  #5eb45e;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row0_col4 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row0_col5 {\n",
       "            background-color:  #3da23d;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row0_col6 {\n",
       "            background-color:  #ade0ad;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row0_col7 {\n",
       "            background-color:  #6abb6a;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row0_col8 {\n",
       "            background-color:  #359d35;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row0_col9 {\n",
       "            background-color:  #76c176;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row0_col10 {\n",
       "            background-color:  #42a442;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row0_col11 {\n",
       "            background-color:  #4ba94b;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row0_col12 {\n",
       "            background-color:  #8bcd8b;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row0_col13 {\n",
       "            background-color:  #309a30;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row0_col14 {\n",
       "            background-color:  #acdfac;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row0_col15 {\n",
       "            background-color:  #97d497;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row0_col16 {\n",
       "            background-color:  #ccf1cc;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row0_col17 {\n",
       "            background-color:  #5fb55f;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row1_col3 {\n",
       "            background-color:  #1b8f1b;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row1_col4 {\n",
       "            background-color:  #53ae53;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row1_col5 {\n",
       "            background-color:  #47a747;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row1_col6 {\n",
       "            background-color:  #b0e2b0;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row1_col7 {\n",
       "            background-color:  #178c17;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row1_col8 {\n",
       "            background-color:  #1c8f1c;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row1_col9 {\n",
       "            background-color:  #068306;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row1_col10 {\n",
       "            background-color:  #249424;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row1_col11 {\n",
       "            background-color:  #5eb45e;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row1_col12 {\n",
       "            background-color:  #8acc8a;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row1_col13 {\n",
       "            background-color:  #48a848;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row1_col14 {\n",
       "            background-color:  #4dab4d;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row1_col15 {\n",
       "            background-color:  #6fbd6f;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row1_col16 {\n",
       "            background-color:  #d8f8d8;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row1_col17 {\n",
       "            background-color:  #a2daa2;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row2_col3 {\n",
       "            background-color:  #a7dda7;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row2_col4 {\n",
       "            background-color:  #259425;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row2_col5 {\n",
       "            background-color:  #88cb88;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row2_col6 {\n",
       "            background-color:  #daf9da;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row2_col7 {\n",
       "            background-color:  #bfeabf;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row2_col8 {\n",
       "            background-color:  #64b764;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row2_col9 {\n",
       "            background-color:  #ade0ad;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row2_col10 {\n",
       "            background-color:  #c2ecc2;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row2_col11 {\n",
       "            background-color:  #81c781;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row2_col12 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row2_col13 {\n",
       "            background-color:  #7ec67e;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row2_col14 {\n",
       "            background-color:  #c2ecc2;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row2_col15 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row2_col16 {\n",
       "            background-color:  #c5edc5;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row2_col17 {\n",
       "            background-color:  #85ca85;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row3_col3 {\n",
       "            background-color:  #b1e2b1;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row3_col4 {\n",
       "            background-color:  #8dce8d;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row3_col5 {\n",
       "            background-color:  #77c277;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row3_col6 {\n",
       "            background-color:  #bbe8bb;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row3_col7 {\n",
       "            background-color:  #96d396;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row3_col8 {\n",
       "            background-color:  #a6dca6;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row3_col9 {\n",
       "            background-color:  #77c277;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row3_col10 {\n",
       "            background-color:  #1a8e1a;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row3_col11 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row3_col12 {\n",
       "            background-color:  #7ac47a;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row3_col13 {\n",
       "            background-color:  #4dab4d;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row3_col14 {\n",
       "            background-color:  #ade0ad;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row3_col15 {\n",
       "            background-color:  #b0e1b0;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row3_col16 {\n",
       "            background-color:  #6fbd6f;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row3_col17 {\n",
       "            background-color:  #4ba94b;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row4_col3 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row4_col4 {\n",
       "            background-color:  #8bcd8b;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row4_col5 {\n",
       "            background-color:  #058305;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row4_col6 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row4_col7 {\n",
       "            background-color:  #319b31;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row4_col8 {\n",
       "            background-color:  #349d34;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row4_col9 {\n",
       "            background-color:  #4daa4d;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row4_col10 {\n",
       "            background-color:  #078407;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row4_col11 {\n",
       "            background-color:  #4daa4d;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row4_col12 {\n",
       "            background-color:  #c1ebc1;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row4_col13 {\n",
       "            background-color:  #339c33;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row4_col14 {\n",
       "            background-color:  #83c983;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row4_col15 {\n",
       "            background-color:  #78c278;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row4_col16 {\n",
       "            background-color:  #3ea23e;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row4_col17 {\n",
       "            background-color:  #3ca13c;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row5_col3 {\n",
       "            background-color:  #a2daa2;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row5_col4 {\n",
       "            background-color:  #b9e7b9;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row5_col5 {\n",
       "            background-color:  #63b763;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row5_col6 {\n",
       "            background-color:  #bde9bd;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row5_col7 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row5_col8 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row5_col9 {\n",
       "            background-color:  #8cce8c;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row5_col10 {\n",
       "            background-color:  #5ab25a;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row5_col11 {\n",
       "            background-color:  #afe1af;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row5_col12 {\n",
       "            background-color:  #0e880e;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row5_col13 {\n",
       "            background-color:  #158b15;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row5_col14 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row5_col15 {\n",
       "            background-color:  #9bd69b;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row5_col16 {\n",
       "            background-color:  #078407;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row5_col17 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row6_col3 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row6_col4 {\n",
       "            background-color:  #b8e6b8;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row6_col5 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row6_col6 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row6_col7 {\n",
       "            background-color:  #d6f7d6;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row6_col8 {\n",
       "            background-color:  #a1d9a1;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row6_col9 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row6_col10 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row6_col11 {\n",
       "            background-color:  #2e992e;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row6_col12 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row6_col13 {\n",
       "            background-color:  #7ac37a;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row6_col14 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row6_col15 {\n",
       "            background-color:  #6fbd6f;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row6_col16 {\n",
       "            background-color:  #caf0ca;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row6_col17 {\n",
       "            background-color:  #8bcd8b;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row7_col3 {\n",
       "            background-color:  #a4dba4;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row7_col4 {\n",
       "            background-color:  #63b763;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row7_col5 {\n",
       "            background-color:  #73c073;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row7_col6 {\n",
       "            background-color:  #289628;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row7_col7 {\n",
       "            background-color:  #339c33;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row7_col8 {\n",
       "            background-color:  #2f9a2f;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row7_col9 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row7_col10 {\n",
       "            background-color:  #55af55;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row7_col11 {\n",
       "            background-color:  #43a543;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row7_col12 {\n",
       "            background-color:  #42a442;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row7_col13 {\n",
       "            background-color:  #399f39;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row7_col14 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row7_col15 {\n",
       "            background-color:  #2c982c;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row7_col16 {\n",
       "            background-color:  #7dc57d;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row7_col17 {\n",
       "            background-color:  #70be70;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row8_col3 {\n",
       "            background-color:  #289628;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row8_col4 {\n",
       "            background-color:  #5eb45e;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row8_col5 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row8_col6 {\n",
       "            background-color:  #daf9da;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row8_col7 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row8_col8 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row8_col9 {\n",
       "            background-color:  #028102;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row8_col10 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row8_col11 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row8_col12 {\n",
       "            background-color:  #399f39;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row8_col13 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row8_col14 {\n",
       "            background-color:  #3aa03a;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row8_col15 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row8_col16 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row8_col17 {\n",
       "            background-color:  #239323;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row9_col3 {\n",
       "            background-color:  #b9e7b9;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row9_col4 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row9_col5 {\n",
       "            background-color:  #d9f8d9;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row9_col6 {\n",
       "            background-color:  #90d090;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row9_col7 {\n",
       "            background-color:  #349d34;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row9_col8 {\n",
       "            background-color:  #45a645;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row9_col9 {\n",
       "            background-color:  #87cb87;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row9_col10 {\n",
       "            background-color:  #c6eec6;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row9_col11 {\n",
       "            background-color:  #4daa4d;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row9_col12 {\n",
       "            background-color:  #399f39;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row9_col13 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row9_col14 {\n",
       "            background-color:  #65b865;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row9_col15 {\n",
       "            background-color:  #98d498;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row9_col16 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }    #T_2190802c_e0a2_11ea_bbfc_f4d108645659row9_col17 {\n",
       "            background-color:  #e5ffe5;\n",
       "            color:  #000000;\n",
       "        }</style><table id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >title</th>        <th class=\"col_heading level0 col1\" >text</th>        <th class=\"col_heading level0 col2\" >link</th>        <th class=\"col_heading level0 col3\" >deep learning</th>        <th class=\"col_heading level0 col4\" >natural language processing</th>        <th class=\"col_heading level0 col5\" >computer vision</th>        <th class=\"col_heading level0 col6\" >statistics</th>        <th class=\"col_heading level0 col7\" >implementation</th>        <th class=\"col_heading level0 col8\" >visualization</th>        <th class=\"col_heading level0 col9\" >industry</th>        <th class=\"col_heading level0 col10\" >software engineering</th>        <th class=\"col_heading level0 col11\" >reddit question</th>        <th class=\"col_heading level0 col12\" >arxiv</th>        <th class=\"col_heading level0 col13\" >cloud computing</th>        <th class=\"col_heading level0 col14\" >deployment</th>        <th class=\"col_heading level0 col15\" >competitions</th>        <th class=\"col_heading level0 col16\" >business</th>        <th class=\"col_heading level0 col17\" >business intelligence</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row0_col0\" class=\"data row0 col0\" >REALM: Retrieval-Augmented Language Model Pre-Training</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row0_col1\" class=\"data row0 col1\" >Natural Language ProcessingAn Introduction to Retrieval-Augmented Language Model Pre-TrainingPhoto by Edward Ma on UnsplashSince 2018, the transformer-based language model has been proven to achieve good performance in lots of NLP downstream tasks such as Open-domain Question Answer (Open-QA). To achieve better results, models intend to increase model parameters (e.g. more heads, larger dimensions) in order to stored world knowledge in the neural network.Guu et al. (2020) from Google Research released the state-of-the-art model (Retrieval-Augmented Language Model Pre-Training, aks REALM) which leverages knowledge retriever augmented data from other large corpora such as Wikipedia. Given an extra signal, it helped the model to deliver a better result. In this storied, we will go through how does this model achieves the start-of-the-art result.REALM OverviewThe overall idea is leveraging extra document to provide more signal to the model such that it can predict masked token accurately. The name this approach as a retrieve-then-predict approach. The following diagram shows pre-trianing workflow.Given a masked sentence (The [MASK] at the top of the pyramid)Feeding a masked sentence to Neural Knowledge Retriever. It will return a document (not necessarily a whole article) that relates to the input.Passing both the original sentence and augmented document to Knowledge-Augmented Encoder. It will predict the masked token (pyramidion).For the fine-tuning stage, it used unmasked sentence instead of a sentence which contains a masked token.Model ArchitectureFrom the previous overview, you may awared that REALM (Guu et al., 2020) contains two models which are knowledge retriever and knowledge-augmented encoder. We will go through it one by one.Knowledge RetrieverFirst of all, the objective of the knowledge retriever is outputting a useful document for the next step. For input, it uses BERT-style to convert the sentence to a token with [CLS] and [SEP] as prefix and prefix respectively. For external documents, it includes both document’s title and body as well. Therefore, we need to concatenate it bye [SEP] which is following BERT-style. You may visit this story for more information about BERT-style format.BERT-styple format. x, x1 and x2 are referring to sentence (Guu et al, 2020)After that, it uses a inner product of the vector embeddings (input and document from knowledge corpus). Softmax will be applied on the inner product result in order to pick the most related document.Knowledge-Augmented EncoderSame as knowledge retreiver, Guu et al. follows BERT mechanism for training and fine-tuning this encoder.In the pre-trianing phase, it uses Masked Language Modeling (Devlin et al., 2018). Basically, the training objective is predicting a masked token by unmasked token. You may visit the story for better understanding on MLM mechanism (Devlin et al., 2018)Example of pre-training phase(unsupervised learning) (Guu et al., 2020)In the Open-QA fine-tuning phase, there is no masked token and Guu et al. assume the answer can be found from document (the output from Knowledge Retriever). It follows BERT-style to construct vector embeddings and passing it to the transformer model.Example of Open-QA fine-tuning phase (supervised learning) (Guu et al., 2020)Maximum Inner Product Search (MIPS)The major challenge of this retrieve-then-predict architecture is selecting a good document from a larger external corpus. Guu et al. proposed to use MIPS to shorter retrieving time.Upper formula: Given input sentence, selecting documents from the corpus. Lower formula: Giving input sentence and selected document, selecting answer (Guu et al., 2020)In order to reduce computation time, Guu et al. proposed a 2 step computation. First of all, calculating the possibility of documents from a larger corpus by providing input sentence x. Leveraging MIPS (Ram and Gary, 2012) to pick top k probability documents as inputs for the next step. MIPS uses build a ball tree to disect data points (i.e. vectors) into differnt cluster. Data points will be splitted into cluster and it will belongs to only one cluster (same level of cluster). Therefore, Guu et al. can use much less running time in order to find top k document.Example of Ball-Tree (Ram and Gray, 2012)Data Processing in Pre-trainingBesides using MIPS to select most relative documents, Guu et al. injects extra information in pre-training to assist model training.Salient SpanAs REALM focus on Open-QA domain, they inteneded to emphasize named entities and dates. Those named entities and dates will be masked as salient spans. To use less effort to figure out named entities, BERT-based tagger is trained in order to identify named eneities and dates.Null DocumentGuu et al. assume that not all masked tokens requrie extra knowledge to predict. Empty document is injected to the top k retrieve documents to similar this situation.Dropout Trivial RetrievalsIt may possible that top k documents include same input sentence. To prevent encoder predict result by focus on unmasked token, this kind of trivial training data will be exlcuded in pre-training phase.Vector InitializationGood vectors lead a better result in predicition. For sake of easier, we may use random initialization but it introduces a cold-start problem. Therefore Guu et al. uses Inverse Cloze Task (ICT) for pre-training of pre-trianing. In short, it is a inverse version of masked token predicition. Giving a query (the left hand side of the below figure), the objective is picking a true context from candidates (the right hand side of the below figure)Example of Inverse Cloze Task (Lee et al., 2019)Take AwaysSalient span for named entities and dates are important. As this model eyes on OpenQA. It is important to let the model to focus on those named entities and dates.Selecting the document from a larger corpus is important. The assumption is that the final result exist in extra documents. It is also important to pick top k related doucments.About MeI am Data Scientist in Bay Area. Focusing on state-of-the-art in Data Science, Artificial Intelligence , especially in NLP and platform related. You can reach me from Medium Blog, LinkedIn or Github.ReferenceP.Ram and A. G. Gray. Maximum Inner-Product Search using Tree Data-structures. 2012Devlin J., Chang M. W., Lee K. and Toutanova K.. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. 2018K. Guu, K. Lee, Z. Tung, P. Pasupat and M. W. Chang. REALM: Retrieval-Augmented Language Model Pre-Training. 2020REALM: Retrieval-Augmented Language Model Pre-Training was originally published in Towards AI — Multidisciplinary Science Journal on Medium, where people are continuing the conversation by highlighting and responding to this story.</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row0_col2\" class=\"data row0 col2\" >https://medium.com/towards-artificial-intelligence/realm-retrieval-augmented-language-model-pre-training-534feae7ab98?source=rss----98111c9905da---4</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row0_col3\" class=\"data row0 col3\" >0.594145</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row0_col4\" class=\"data row0 col4\" >0.613702</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row0_col5\" class=\"data row0 col5\" >0.534048</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row0_col6\" class=\"data row0 col6\" >0.502438</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row0_col7\" class=\"data row0 col7\" >0.564319</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row0_col8\" class=\"data row0 col8\" >0.550380</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row0_col9\" class=\"data row0 col9\" >0.517819</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row0_col10\" class=\"data row0 col10\" >0.585611</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row0_col11\" class=\"data row0 col11\" >0.549369</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row0_col12\" class=\"data row0 col12\" >0.490301</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row0_col13\" class=\"data row0 col13\" >0.518406</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row0_col14\" class=\"data row0 col14\" >0.505397</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row0_col15\" class=\"data row0 col15\" >0.519020</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row0_col16\" class=\"data row0 col16\" >0.496123</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row0_col17\" class=\"data row0 col17\" >0.518821</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row1_col0\" class=\"data row1 col0\" >Evaluating the Impact of Knowledge Graph Context on Entity Disambiguation Models</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row1_col1\" class=\"data row1 col1\" >Pretrained Transformer models have emerged as state-of-the-art approaches that learn contextual information from the text to improve the performance of several NLP tasks. Code: https://github.com/mulangonando/Impact-of-KG-Context-on-ED</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row1_col2\" class=\"data row1 col2\" >https://paperswithcode.com/paper/evaluating-the-impact-of-knowledge-graph</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row1_col3\" class=\"data row1 col3\" >0.613387</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row1_col4\" class=\"data row1 col4\" >0.597722</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row1_col5\" class=\"data row1 col5\" >0.531973</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row1_col6\" class=\"data row1 col6\" >0.501481</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row1_col7\" class=\"data row1 col7\" >0.593792</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row1_col8\" class=\"data row1 col8\" >0.559076</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row1_col9\" class=\"data row1 col9\" >0.545501</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row1_col10\" class=\"data row1 col10\" >0.592912</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row1_col11\" class=\"data row1 col11\" >0.543777</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row1_col12\" class=\"data row1 col12\" >0.490492</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row1_col13\" class=\"data row1 col13\" >0.511187</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row1_col14\" class=\"data row1 col14\" >0.537554</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row1_col15\" class=\"data row1 col15\" >0.530916</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row1_col16\" class=\"data row1 col16\" >0.494104</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row1_col17\" class=\"data row1 col17\" >0.496908</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row2_col0\" class=\"data row2 col0\" >[P] Malay and Indonesian languages Toolkit, malaya.readthedocs.io</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row2_col1\" class=\"data row2 col1\" >Malaya is a Natural-Language-Toolkit library for Malay and Indonesian languages, powered by Deep Learning Tensorflow, https://malaya.readthedocs.io/en/latest/ Modules  Augmentation  Augment any text using dictionary of synonym, Wordvector or Transformer-Bahasa.  Constituency Parsing  Transfer learning on BERT-base-bahasa, Tiny-BERT-bahasa, Albert-base-bahasa, Albert-tiny-bahasa, XLNET-base-bahasa.  Dependency Parsing  Transfer learning on BERT-base-bahasa, Tiny-BERT-bahasa, Albert-base-bahasa, Albert-tiny-bahasa, XLNET-base-bahasa, ALXLNET-base-bahasa.  Emotion Analysis  Transfer learning on BERT-base-bahasa, Tiny-BERT-bahasa, Albert-base-bahasa, Albert-tiny-bahasa, XLNET-base-bahasa, ALXLNET-base-bahasa.  Entities Recognition  Transfer learning on BERT-base-bahasa, Tiny-BERT-bahasa, Albert-base-bahasa, Albert-tiny-bahasa, XLNET-base-bahasa, ALXLNET-base-bahasa.  Generator  Generate any texts given a context using T5-Bahasa, GPT2-Bahasa or Transformer-Bahasa.  Keyword Extraction  Provide RAKE, TextRank and Attention Mechanism hybrid with Transformer-Bahasa.  Language Detection  using Fast-text and Sparse Deep learning Model to classify Malay (formal and social media), Indonesia (formal and social media), Rojak language and Manglish.  Normalizer  using local Malaysia NLP researches hybrid with Transformer-Bahasa to normalize any bahasa texts.  Num2Word  Convert from numbers to cardinal or ordinal representation.  Paraphrase  Provide Abstractive Paraphrase using T5-Bahasa and Transformer-Bahasa.  Part-of-Speech Recognition  Transfer learning on BERT-base-bahasa, Tiny-BERT-bahasa, Albert-base-bahasa, Albert-tiny-bahasa, XLNET-base-bahasa, ALXLNET-base-bahasa.  Relevancy Analysis  Transfer learning on BERT-base-bahasa, Tiny-BERT-bahasa, Albert-base-bahasa, Albert-tiny-bahasa, XLNET-base-bahasa, ALXLNET-base-bahasa.  Sentiment Analysis  Transfer learning on BERT-base-bahasa, Tiny-BERT-bahasa, Albert-base-bahasa, Albert-tiny-bahasa, XLNET-base-bahasa, ALXLNET-base-bahasa.  Similarity  Using deep Encoder, Doc2Vec, BERT-base-bahasa, Tiny-BERT-bahasa, Albert-base-bahasa, Albert-tiny-bahasa, XLNET-base-bahasa and ALXLNET-base-bahasa to build deep semantic similarity models.  Spell Correction  Using local Malaysia NLP researches hybrid with Transformer-Bahasa to auto-correct any bahasa words.  Stemmer  Using BPE LSTM Seq2Seq with attention state-of-art to do Bahasa stemming.  Subjectivity Analysis  Transfer learning on BERT-base-bahasa, Tiny-BERT-bahasa, Albert-base-bahasa, Albert-tiny-bahasa, XLNET-base-bahasa, ALXLNET-base-bahasa.  Summarization  Provide Abstractive T5-Bahasa also Extractive interface using Transformer-Bahasa, skip-thought, LDA, LSA and Doc2Vec.  Topic Modelling  Provide Transformer-Bahasa, LDA2Vec, LDA, NMF and LSA interface for easy topic modelling with topics visualization.  Toxicity Analysis  Transfer learning on BERT-base-bahasa, Tiny-BERT-bahasa, Albert-base-bahasa, Albert-tiny-bahasa, XLNET-base-bahasa, ALXLNET-base-bahasa.  Transformer  Provide easy interface to load BERT-base-bahasa, Tiny-BERT-bahasa, Albert-base-bahasa, Albert-tiny-bahasa, XLNET-base-bahasa, ALXLNET-base-bahasa, ELECTRA-base-bahasa and ELECTRA-small-bahasa.  Translation  provide Neural Machine Translation using Transformer for EN to MS and MS to EN.  Word2Num  Convert from cardinal or ordinal representation to numbers.  Word2Vec  Provide pretrained bahasa wikipedia and bahasa news Word2Vec, with easy interface and visualization.  Zero-shot classification  Provide Zero-shot classification interface using Transformer-Bahasa to recognize texts without any labeled training data. Pretrained Models Malaya also released Bahasa pretrained models, simply check at https://github.com/huseinzol05/Malaya/tree/master/pretrained-model Or can try use huggingface 🤗 Transformers library, https://huggingface.co/models?filter=ms    submitted by    /u/huseinzol05   [link] [comments]</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row2_col2\" class=\"data row2 col2\" >https://www.reddit.com/r/MachineLearning/comments/iav8g5/p_malay_and_indonesian_languages_toolkit/</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row2_col3\" class=\"data row2 col3\" >0.573247</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row2_col4\" class=\"data row2 col4\" >0.606518</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row2_col5\" class=\"data row2 col5\" >0.518175</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row2_col6\" class=\"data row2 col6\" >0.492254</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row2_col7\" class=\"data row2 col7\" >0.534429</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row2_col8\" class=\"data row2 col8\" >0.534387</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row2_col9\" class=\"data row2 col9\" >0.504195</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row2_col10\" class=\"data row2 col10\" >0.553177</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row2_col11\" class=\"data row2 col11\" >0.534189</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row2_col12\" class=\"data row2 col12\" >0.505804</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row2_col13\" class=\"data row2 col13\" >0.495088</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row2_col14\" class=\"data row2 col14\" >0.497745</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row2_col15\" class=\"data row2 col15\" >0.496114</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row2_col16\" class=\"data row2 col16\" >0.497304</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row2_col17\" class=\"data row2 col17\" >0.506360</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row3_col0\" class=\"data row3 col0\" >A single legal text representation at Doctrine: the legal camemBERT</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row3_col1\" class=\"data row3 col1\" >As a legal platform, Doctrine aggregates a lot of legal data with the intent of making them accessible, understandable and usable. The Machine Learning Engineers’ day-to-day material is mostly text: court decisions, legislation, legal commentaries, user queries, etc. All of our content is natural language, which we process in a number of ways: bag-of-words, embeddings or with language models.In an ideal world though, our product would be built on top of scalable, flexible and reusable modules, ones that would be generic enough to accommodate a wide variety of legal contents and feed the whole spectrum of our product features. It is exactly with that vision in mind that we started working on a unified language model a few months ago, whose associated challenges, findings and results we’ll do our best to summarize in this article.I. One language model to rule them allDepending on the project, we were representing our legal contents with:different techniques:TF-IDF vectorsBM25 (e.g., with ElasticSearch)A variant of Word2Vec, called Wang2Vec, embeddings fine-tuned on legal data — note that even if those embeddings work pretty well for a lot of tasks, they are not the state-of-the-art anymore. There’s not enough modeling power in simple word embeddings and we definitely see their limits now on some tasks.2. different data:vocabulary of the content itself,vocabulary of the linked contents from our legal graphvocabulary from some metadata provided by the courts…Yet eventually, we want to be able to represent all of our legal content using a unified framework for any text-understanding based feature, because of:Reusability: all teams can rely on this unique language model for their projects.2. Scalability:a modeling power sufficient to be applied to any new legal content (e.g., legal documents from the lower house and the upper house),robust enough to unlock use cases we’re not yet considering, like legal bots, legal trend detection, argument mining, etc,generic enough to be applied to a new language (with a retraining on the new language of course).3. Agnostic usage: one of the problems with our current representations is that the text follows some guidelines in the way they phrase statements, and a textual similarity is thus strongly biased towards documents that have the same overall phrasing (of the same court for example), despite the fact they’re not invoking the same laws about the same thing. For example, it is now difficult for us to match decisions from the High Court/Court of Appeal to those from the Supreme Court simply because of their different writing styles (the former tends to focus primarily and precisely on the facts, while the latter favors usually only relies on the legal matter, which has an adverse effect on our current representations).When we initially started thinking about this, there were some properties that we thought our language model should ideally cover:Taking advantage of the semantic proximity:In French:préjudice corporel should be equivalent to dommage corporelIn English: death should be equivalent to loss of life2. Being able to represent our content on different granularities:Token-level for Named Entity Recognition: anonymization, entity detection, …Paragraph-level: structure detection, argument similarities, …Document-level: legal domain classification, document recommendation, …It’s with all those things in mind that we started to work on a unique, all-encompassing language model serving all our use cases and features.II. Our legal language modelThe first step of this project was to design the architecture and implementation of our language model. This step was crucial since it would serve as the foundation to all of our future work and help us move towards our initial vision. We first thought about our technical constraints:use an existing and robust implementation, in order to take advantage of the support and the community,use a state-of-the-art technique to achieve very good performances,ideally use a PyTorch implementation, because our previous Deep Learning algorithms were made with PyTorch. Moreover, PyTorch (along with a few others) remains the dominant deep learning library at the time of writing this article,if possible, find an implementation with a French pre-trained model before fine-tuning, because transfer learning has shown its efficiency in NLP.It should also be noted that compared to other use-cases, especially in academic research, the framework should be efficient at representing very long texts. Here is an interesting blog post about different document embeddings techniques. We’ll come to that later.Under these constraints, the Hugging Face Transformers library appeared to be a very good choice:they offer all the recent state-of-the-art architectures (BERT, RoBERTa, ELMo, XLNet, …) complete with their associated PyTorch and TensorFlow implementations,some of them have a French pre-trained model,their implementation has quickly become an international reference, to the point where the famous NLP framework Spacy provides a Transformer implementation based on the Hugging Face one.Among the models providing a French pre-trained model, we had the choice between:BERT-Base, multilingualDistilmBERT, multilingualcamemBERT, French RoBERTa modelWe decided to go for camemBERT, since it already provided good results for the French language on several tasks according to this paper. Of course, multilingual models will probably be very useful for internationalization later, but we initially wanted to check that a transformer model could be relevant. Moreover, camemBERT has fewer parameters than multilingual models, which makes it a little easier to use.Note that camemBERT is case-sensitive, which will be useful for Named Entity Recognition and especially for anonymization.The legal CamemBERTNow that we had settled on the underlying technology, we decided to check how well it would perform on actual, real-life legal data.Knowing that camemBERT was initially trained on the French subcorpus of OSCAR, which features gigabytes of data crawled from the web, we knew that it would fare well at general French language tasks, but we suspected that the task of speaking the more specific French legalese would prove to be a tougher nut to crack, which our initial tests confirmed.For example, when asked to predict the next word of the sentence Par ces ... , camemBERT suggested the word mots, which is not exactly legal-oriented. We would expect something like moyens or motifs.It was obvious at this point that the trove of millions of legal documents we have at our disposal at Doctrine would prove to be great material for the subsequent fine-tuning needed to harness the full power of our model. At this point, we were confident that the model could be trained, however, we needed it to be potentially used universally across features. Yet, one issue remained: how to handle long texts, a strong prerequisite for legal documents, but something that doesn’t pair naturally with transformers’ inherent limitations.BERT models, for example, have a hard limit of 512 to 514 tokens (as enforced by the max_position_embeddings parameter), which would surely be a challenge when dealing with court decisions: texts that can be infamously verbose, with an average token count hovering around 2000 (and some even more extreme cases like this decision).To circumvent this issue, we envisioned two different approaches:Embedding each paragraphHaving sliding windows, as explained hereTo avoid ending up with redundancy in the embeddings, we decided to go with paragraph embeddings first, with exceedingly long paragraphs getting snipped past the limit during training. What was left for us to determine at that point was an aggregation strategy over the different paragraphs, so that we could harvest the final document embeddings, something that we would come back to later.We then proceeded with the implementation, which was done by splitting our legal documents on paragraphs and fine-tuning camemBERT on the masked language model task (using dedicated AWS GPU instances). It converged after a few days and we tested its relevance by using a few qualitative checks:Comparison between the standard pre-trained French camemBERT model and our legal camemBERT on a masked LM taskWe assessed the differences in prediction for semantically similar sentences, which seemed to be consistent. The qualitative check seemed to provide very good results. It was now time to validate the language model on a real task.III. Our first legal camemBERT use-case: classification of legal domainWe wanted to try our legal camemBERT on a simple task for a first validation: text classification of legal domains on court decisions.This is indeed a simple and well delimited task, and easy to compare to other basic models. Moreover, this classification has a huge product impact, on the search filters, recommender systems and analytics.We have two hierarchies on the legal domains at Doctrine:the main legal domain:Droit civil,Droit commercial,Droit social,Droit public,…2. the subdomain: for example in Droit civil, there areDivorce et séparation de corpsDroit locatifDroit des successionsDroit de la responsabilité…Today, we support 9 different domains and 40 different subdomains, where some are more complex than others to determine. These categories have a hierarchical structure, but we addressed the problem by reducing it to a 40-class classification problem.The HuggingFace repository suggests a classification head module integrated with CamemBERT. However, as discussed earlier, the main problem is that court decisions can be very verbose (have a look at this very long decision for example), and BERT does not work well on long texts. A very good review of document embeddings showed that there are no clear embedding technique that works better than others for very long documents. It really depends on your objective.Working at a paragraph level seemed more relevant, all the more so as the language model has been trained at a paragraph scale. BERT will then provide an embedding for each paragraph. We then had to think about a way to aggregate the paragraphs in order to get a decision embedding.ModelingParagraph embeddings methodIt is known that BERT architectures provide not only word-level contextual embeddings but also the special CLS-token whose output embedding is used for classification tasks. However it turns out to be a poor embedding of the input sequence of other tasks if not fine-tuned on the specific task:The paper Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks from Reimers et al, 2019, shows that BERT out-of-the-box maps sentences to a vector space that is rather unsuitable to be used with common similarity measures like cosine-similarity.According to BERT creator Jacob Devlin: “I’m not sure what these vectors are, since BERT does not generate meaningful sentence vectors. It seems that this is is doing average pooling over the word tokens to get a sentence vector, but we never suggested that this will generate meaningful sentence representations.” sourceStill, the most classic ways to embed a document (in our case, a paragraph) with BERT are:to use the [CLS]-tokento use an aggregation of the last X hidden states of the word embeddings ( we usually saw X=4)What is interesting in our case is that one paragraph does not represent the whole court decision. We had to plug something on top of it. We decided to go with the [CLS]-token as paragraph embeddings for a first shot, because our task is a classification task.2. Document embedding with an aggregation over paragraphsGiven embeddings for all our paragraphs, we then had to think of a way to get document embeddings.Here again, different approaches can be considered, since this is another sequence-to-one vector modeling:A simple average of all paragraph embeddings (the [CLS]-token of each BERT-output paragraphs),A weighted average of the paragraph embeddings, with weights built with a self-attention mechanism explained in the paper A Structured Self-attentive Sentence Embedding,A bi-LSTM to exploit the sequential information contained in the paragraphs,A Convolutional Neural Network,Another BERT that would learn the language at the paragraph scale,…Given that our task is a mere classification problem, the solution with a self-attention mechanism seemed to be pretty relevant for our case because:It’s a bit smarter than a simple average-pooling, and it will automatically get rid of the useless paragraphs that contain no information for the legal domain. Indeed, the final paragraphs of French decisions are often related to the operative part of the judgment, and about who pays the costs. This is usually not relevant to our current problem.It also provides some precious insights on how to best interpret the model. We can indeed have access to the attention weights and check on which paragraphs the model focused on the most for its prediction.With all that mind, here’s the final architecture for the classification task:Final architecture of our legal document classification on documents, using the legal camemBERTWe first tried to train the whole pipeline, including the fine-tuning of the legal camemBERT on this task, but we got memory errors. We quickly froze the BERT model and trained only the rest of the pipeline (attention layer + classification layer). It provided good results so we didn’t go with further experiments on an end-to-end training. This is something that we made a note of though, since unsupervised BERT outputs are known to be poor if not fine-tuned, as discussed earlier in this article.ResultsThe goal here was not only to improve our legal domains classification, but also to show that we could achieve at least the same results as a simple TF-IDF model.Dataset creationDeep learning in general often requires a consequent training set size. That’s why we used a semi-automatically labelled training dataset, labelled:by humans, using Prodi.gywith business rules, using the associated court as a reference. If a decision is linked to another one from Labor court, it’s very likely that the decision is about Droit du travail(labor laws).with the most reliable predictions of our former algorithm, based on TF-IDF for the domain, and a legal taxonomy for the subdomain.Comparison between models and discussionWe achieved the same performance with our legal camemBERT and with a simple TF-IDF, which is actually good news! We indeed didn’t spend a lot of time on the modeling part of camemBERT, and this classification task is in the end a rather simple NLP task.Moreover and perhaps just as interestingly, we noticed after a qualitative analysis of model’s prediction errors that the errors of the simple model were more often out of context. It means that when the TF-IDF gets it wrong, it’s really way off the mark. For example, this decision is predicted as Droit du transport with a probability of 0.96, instead of Droit des assurances because the decision is about a vehicle insurance claim and contains a lot of vocabulary related to transportation, and not that much about insurance.On the other hand, the legal camemBERT can of course be wrong, but it never steers too much out of context and will mostly predict subdomains that are very close, like Droit immobilier et de la construction and Droit de la copropriété et de la propriété immobilière, when we look at the confusion matrix.Moreover, CamemBERT managed to predict some subdomains that were not obvious at all, even for humans. For example, this decision has been predicted as Divorce et séparations de corpswithout any explicit mention of the word divorce in the decision! The subdomain here is very implicit and implied by a mention to a father that has to pay alimony to the mother of his child.Let’s now have a look at the attention weights of our modeling. Here are some examples below:Paragraph with the highest attention score (0.34) for the prediction of https://www.doctrine.fr/d/CA/Reims/2008/SK60FC7292250FC0B001E6 as Divorce et Séparation de corpsParagraph with the highest attention score (0.26) for the prediction of https://www.doctrine.fr/d/CA/Rouen/2016/1F43DFAE32435B18DC90 as Droit des étrangers et de la nationalitéThese attention scores totally make sense, and confirmed the approach.We also confirmed that paragraphs related to generic procedures had a very low attention weight, like this one:Paragraph with a very low attention weight of 0.01 for the prediction of https://www.doctrine.fr/d/CA/Rouen/2016/1F43DFAE32435B18DC90 as Droit des étrangers et de la nationalitéFinally, when we had a look at the errors of the models (both models), we quickly noticed that some classes were very well predicted and some others were not. Our intuition about the observed discrepancy boils down to the fact that language models are only ever as good as their training dataset. In our case, the issue seems to stem from volume and errors in the training set. This is definitely the next priority for this task to focus on, before trying to play with the different architectures. Indeed, the current one seems to work pretty well on subdomains when the training dataset is satisfactory.ConclusionWe built a legal language model with a state-of-the-art technique, that proved to be very efficient at capturing highly relevant information on a simple classification task. This is a huge step for Doctrine, as we have a lot of very complex tasks in Natural Language Processing to tackle! The granularity of this new language model, which can seamlessly provide token, paragraph and document embeddings will be key for us to find new applications for the technique on a wide array of complex Natural Language Processing tasks at Doctrine.In fact, the legal camemBERT has already found a second problem to tackle with the issue of semantic similarity between users and legal content in the context of a recommendation system and seems to already have yielded promising results, which we’ll be sharing in an upcoming blog post very soon. Stay tuned!A single legal text representation at Doctrine: the legal camemBERT was originally published in Inside Doctrine on Medium, where people are continuing the conversation by highlighting and responding to this story.</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row3_col2\" class=\"data row3 col2\" >https://medium.com/doctrine/a-single-legal-text-representation-at-doctrine-the-legal-camembert-a5ee2b851763?source=rss----8986dd8e9bad---4</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row3_col3\" class=\"data row3 col3\" >0.570231</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row3_col4\" class=\"data row3 col4\" >0.586314</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row3_col5\" class=\"data row3 col5\" >0.521751</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row3_col6\" class=\"data row3 col6\" >0.499045</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row3_col7\" class=\"data row3 col7\" >0.548911</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row3_col8\" class=\"data row3 col8\" >0.511261</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row3_col9\" class=\"data row3 col9\" >0.517473</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row3_col10\" class=\"data row3 col10\" >0.595577</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row3_col11\" class=\"data row3 col11\" >0.505991</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row3_col12\" class=\"data row3 col12\" >0.492187</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row3_col13\" class=\"data row3 col13\" >0.509414</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row3_col14\" class=\"data row3 col14\" >0.505175</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row3_col15\" class=\"data row3 col15\" >0.512034</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row3_col16\" class=\"data row3 col16\" >0.511341</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row3_col17\" class=\"data row3 col17\" >0.525712</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row4_col0\" class=\"data row4 col0\" >A Large-Scale Chinese Short-Text Conversation Dataset</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row4_col1\" class=\"data row4 col1\" >The cleaned dataset and the pre-training models will facilitate the research of short-text conversation modeling. Code: https://github.com/thu-coai/CDial-GPT</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row4_col2\" class=\"data row4 col2\" >https://paperswithcode.com/paper/a-large-scale-chinese-short-text-conversation</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row4_col3\" class=\"data row4 col3\" >0.621184</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row4_col4\" class=\"data row4 col4\" >0.586691</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row4_col5\" class=\"data row4 col5\" >0.545824</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row4_col6\" class=\"data row4 col6\" >0.541583</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row4_col7\" class=\"data row4 col7\" >0.584535</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row4_col8\" class=\"data row4 col8\" >0.550647</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row4_col9\" class=\"data row4 col9\" >0.527896</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row4_col10\" class=\"data row4 col10\" >0.600209</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row4_col11\" class=\"data row4 col11\" >0.548811</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row4_col12\" class=\"data row4 col12\" >0.484411</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row4_col13\" class=\"data row4 col13\" >0.517256</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row4_col14\" class=\"data row4 col14\" >0.519267</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row4_col15\" class=\"data row4 col15\" >0.528229</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row4_col16\" class=\"data row4 col16\" >0.519199</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row4_col17\" class=\"data row4 col17\" >0.530539</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row5_col0\" class=\"data row5 col0\" >[D] Why does models like GPT-3 or BERT don't have overfitting problems?</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row5_col1\" class=\"data row5 col1\" >Hey everyone, I am new to Natural Language Processing, but I have experience in Machine Learning and Convolutional Neural Network. While reading the GPT-3 paper, this question came to my mind, like having around 175 billion trainable the equation that will come out must be very complex and also it is trained on such a huge dataset. Than why is their no case of overfitting on this model.    submitted by    /u/psarangi112   [link] [comments]</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row5_col2\" class=\"data row5 col2\" >https://www.reddit.com/r/MachineLearning/comments/ib4rth/d_why_does_models_like_gpt3_or_bert_dont_have/</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row5_col3\" class=\"data row5 col3\" >0.574753</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row5_col4\" class=\"data row5 col4\" >0.577933</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row5_col5\" class=\"data row5 col5\" >0.525968</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row5_col6\" class=\"data row5 col6\" >0.498621</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row5_col7\" class=\"data row5 col7\" >0.520720</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row5_col8\" class=\"data row5 col8\" >0.489520</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row5_col9\" class=\"data row5 col9\" >0.512251</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row5_col10\" class=\"data row5 col10\" >0.579364</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row5_col11\" class=\"data row5 col11\" >0.521330</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row5_col12\" class=\"data row5 col12\" >0.504109</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row5_col13\" class=\"data row5 col13\" >0.526190</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row5_col14\" class=\"data row5 col14\" >0.485646</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row5_col15\" class=\"data row5 col15\" >0.517864</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row5_col16\" class=\"data row5 col16\" >0.528094</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row5_col17\" class=\"data row5 col17\" >0.550471</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row6_col0\" class=\"data row6 col0\" >[Q] Data scientist here, working on gathering a corpus of academic papers focusing on \"Cognitive Linguistics\". Need your help!</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row6_col1\" class=\"data row6 col1\" >Hello. I want to collect as many as papers as I can that will fall into this category. The main problem is that the \"tagging\" is not consistent for linguistic papers. Hence I'm looking for an exhausitve list of tags which are directly related to this field, in order to make better queries and find more relevant data.  Thanks!    submitted by    /u/quit_daedalus   [link] [comments]</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row6_col2\" class=\"data row6 col2\" >https://www.reddit.com/r/cognitivelinguistics/comments/ensu0n/q_data_scientist_here_working_on_gathering_a/</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row6_col3\" class=\"data row6 col3\" >0.555300</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row6_col4\" class=\"data row6 col4\" >0.578011</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row6_col5\" class=\"data row6 col5\" >0.498356</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row6_col6\" class=\"data row6 col6\" >0.489413</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row6_col7\" class=\"data row6 col7\" >0.526189</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row6_col8\" class=\"data row6 col8\" >0.513210</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row6_col9\" class=\"data row6 col9\" >0.489906</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row6_col10\" class=\"data row6 col10\" >0.544217</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row6_col11\" class=\"data row6 col11\" >0.557324</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row6_col12\" class=\"data row6 col12\" >0.480271</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row6_col13\" class=\"data row6 col13\" >0.496461</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row6_col14\" class=\"data row6 col14\" >0.485666</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row6_col15\" class=\"data row6 col15\" >0.530694</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row6_col16\" class=\"data row6 col16\" >0.496379</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row6_col17\" class=\"data row6 col17\" >0.504614</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row7_col0\" class=\"data row7 col0\" >KR-BERT: A Small-Scale Korean-Specific Language Model</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row7_col1\" class=\"data row7 col1\" >Since the appearance of BERT, recent works including XLNet and RoBERTa utilize sentence embedding models pre-trained by large corpora and a large number of parameters. Code: https://github.com/snunlp/KR-BERT</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row7_col2\" class=\"data row7 col2\" >https://paperswithcode.com/paper/kr-bert-a-small-scale-korean-specific</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row7_col3\" class=\"data row7 col3\" >0.574174</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row7_col4\" class=\"data row7 col4\" >0.594593</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row7_col5\" class=\"data row7 col5\" >0.522638</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row7_col6\" class=\"data row7 col6\" >0.532562</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row7_col7\" class=\"data row7 col7\" >0.583562</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row7_col8\" class=\"data row7 col8\" >0.552562</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row7_col9\" class=\"data row7 col9\" >0.547111</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row7_col10\" class=\"data row7 col10\" >0.580898</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row7_col11\" class=\"data row7 col11\" >0.551625</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row7_col12\" class=\"data row7 col12\" >0.498453</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row7_col13\" class=\"data row7 col13\" >0.515522</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row7_col14\" class=\"data row7 col14\" >0.564181</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row7_col15\" class=\"data row7 col15\" >0.550184</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row7_col16\" class=\"data row7 col16\" >0.508991</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row7_col17\" class=\"data row7 col17\" >0.513304</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row8_col0\" class=\"data row8 col0\" >Dialogue State Induction Using Neural Latent Variable Models</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row8_col1\" class=\"data row8 col1\" >Dialogue state modules are a useful component in a task-oriented dialogue system. Code: https://github.com/taolusi/dialogue-state-induction</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row8_col2\" class=\"data row8 col2\" >https://paperswithcode.com/paper/dialogue-state-induction-using-neural-latent</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row8_col3\" class=\"data row8 col3\" >0.609547</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row8_col4\" class=\"data row8 col4\" >0.595336</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row8_col5\" class=\"data row8 col5\" >0.546996</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row8_col6\" class=\"data row8 col6\" >0.492073</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row8_col7\" class=\"data row8 col7\" >0.601822</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row8_col8\" class=\"data row8 col8\" >0.568787</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row8_col9\" class=\"data row8 col9\" >0.546473</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row8_col10\" class=\"data row8 col10\" >0.602195</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row8_col11\" class=\"data row8 col11\" >0.570310</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row8_col12\" class=\"data row8 col12\" >0.499499</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row8_col13\" class=\"data row8 col13\" >0.532563</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row8_col14\" class=\"data row8 col14\" >0.543948</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row8_col15\" class=\"data row8 col15\" >0.563158</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row8_col16\" class=\"data row8 col16\" >0.529331</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row8_col17\" class=\"data row8 col17\" >0.538693</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row9_col0\" class=\"data row9 col0\" >[D] Supervised Approach to Extractive Text Summarisation | Research Paper Walkthrough</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row9_col1\" class=\"data row9 col1\" >Automatic text summarisation is the task of shortening the given piece of text yet retaining the main crux and flow. Check out the new paper explainer video 🔥  Check out - https://youtu.be/73uWfopdjoc Original Paper - https://www.aclweb.org/anthology/K17-1021/    submitted by    /u/prakhar21   [link] [comments]</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row9_col2\" class=\"data row9 col2\" >https://www.reddit.com/r/MachineLearning/comments/ibbo60/d_supervised_approach_to_extractive_text/</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row9_col3\" class=\"data row9 col3\" >0.568026</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row9_col4\" class=\"data row9 col4\" >0.569306</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row9_col5\" class=\"data row9 col5\" >0.501154</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row9_col6\" class=\"data row9 col6\" >0.508824</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row9_col7\" class=\"data row9 col7\" >0.583346</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row9_col8\" class=\"data row9 col8\" >0.544889</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row9_col9\" class=\"data row9 col9\" >0.513401</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row9_col10\" class=\"data row9 col10\" >0.552232</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row9_col11\" class=\"data row9 col11\" >0.548748</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row9_col12\" class=\"data row9 col12\" >0.499457</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row9_col13\" class=\"data row9 col13\" >0.464308</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row9_col14\" class=\"data row9 col14\" >0.529640</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row9_col15\" class=\"data row9 col15\" >0.518805</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row9_col16\" class=\"data row9 col16\" >0.491907</td>\n",
       "                        <td id=\"T_2190802c_e0a2_11ea_bbfc_f4d108645659row9_col17\" class=\"data row9 col17\" >0.474505</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f4965e87110>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df.style.background_gradient(cmap=cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural Language ProcessingAn Introduction to Retrieval-Augmented Language Model Pre-TrainingPhoto by Edward Ma on\\xa0UnsplashSince 2018, the transformer-based language model has been proven to achieve good performance in lots of NLP downstream tasks such as Open-domain Question Answer (Open-QA). To achieve better results, models intend to increase model parameters (e.g. more heads, larger dimensions) in order to stored world knowledge in the neural\\xa0network.Guu et al. (2020) from Google Research released the state-of-the-art model (Retrieval-Augmented Language Model Pre-Training, aks REALM) which leverages knowledge retriever augmented data from other large corpora such as Wikipedia. Given an extra signal, it helped the model to deliver a better result. In this storied, we will go through how does this model achieves the start-of-the-art result.REALM OverviewThe overall idea is leveraging extra document to provide more signal to the model such that it can predict masked token accurately. The name this approach as a retrieve-then-predict approach. The following diagram shows pre-trianing workflow.Given a masked sentence (The [MASK] at the top of the\\xa0pyramid)Feeding a masked sentence to Neural Knowledge Retriever. It will return a document (not necessarily a whole article) that relates to the\\xa0input.Passing both the original sentence and augmented document to Knowledge-Augmented Encoder. It will predict the masked token (pyramidion).For the fine-tuning stage, it used unmasked sentence instead of a sentence which contains a masked\\xa0token.Model ArchitectureFrom the previous overview, you may awared that REALM (Guu et al., 2020) contains two models which are knowledge retriever and knowledge-augmented encoder. We will go through it one by\\xa0one.Knowledge RetrieverFirst of all, the objective of the knowledge retriever is outputting a useful document for the next step. For input, it uses BERT-style to convert the sentence to a token with [CLS] and [SEP] as prefix and prefix respectively. For external documents, it includes both document’s title and body as well. Therefore, we need to concatenate it bye [SEP] which is following BERT-style. You may visit this story for more information about BERT-style format.BERT-styple format. x, x1 and x2 are referring to sentence (Guu et al,\\xa02020)After that, it uses a inner product of the vector embeddings (input and document from knowledge corpus). Softmax will be applied on the inner product result in order to pick the most related document.Knowledge-Augmented EncoderSame as knowledge retreiver, Guu et al. follows BERT mechanism for training and fine-tuning this\\xa0encoder.In the pre-trianing phase, it uses Masked Language Modeling (Devlin et al., 2018). Basically, the training objective is predicting a masked token by unmasked token. You may visit the story for better understanding on MLM mechanism (Devlin et al.,\\xa02018)Example of pre-training phase(unsupervised learning) (Guu et al.,\\xa02020)In the Open-QA fine-tuning phase, there is no masked token and Guu et al. assume the answer can be found from document (the output from Knowledge Retriever). It follows BERT-style to construct vector embeddings and passing it to the transformer model.Example of Open-QA fine-tuning phase (supervised learning) (Guu et al.,\\xa02020)Maximum Inner Product Search\\xa0(MIPS)The major challenge of this retrieve-then-predict architecture is selecting a good document from a larger external corpus. Guu et al. proposed to use MIPS to shorter retrieving time.Upper formula: Given input sentence, selecting documents from the corpus. Lower formula: Giving input sentence and selected document, selecting answer (Guu et al.,\\xa02020)In order to reduce computation time, Guu et al. proposed a 2 step computation. First of all, calculating the possibility of documents from a larger corpus by providing input sentence x. Leveraging MIPS (Ram and Gary, 2012) to pick top k probability documents as inputs for the next step. MIPS uses build a ball tree to disect data points (i.e. vectors) into differnt cluster. Data points will be splitted into cluster and it will belongs to only one cluster (same level of cluster). Therefore, Guu et al. can use much less running time in order to find top k document.Example of Ball-Tree (Ram and Gray,\\xa02012)Data Processing in Pre-trainingBesides using MIPS to select most relative documents, Guu et al. injects extra information in pre-training to assist model training.Salient SpanAs REALM focus on Open-QA domain, they inteneded to emphasize named entities and dates. Those named entities and dates will be masked as salient spans. To use less effort to figure out named entities, BERT-based tagger is trained in order to identify named eneities and\\xa0dates.Null DocumentGuu et al. assume that not all masked tokens requrie extra knowledge to predict. Empty document is injected to the top k retrieve documents to similar this situation.Dropout Trivial RetrievalsIt may possible that top k documents include same input sentence. To prevent encoder predict result by focus on unmasked token, this kind of trivial training data will be exlcuded in pre-training phase.Vector InitializationGood vectors lead a better result in predicition. For sake of easier, we may use random initialization but it introduces a cold-start problem. Therefore Guu et al. uses Inverse Cloze Task (ICT) for pre-training of pre-trianing. In short, it is a inverse version of masked token predicition. Giving a query (the left hand side of the below figure), the objective is picking a true context from candidates (the right hand side of the below\\xa0figure)Example of Inverse Cloze Task (Lee et al.,\\xa02019)Take AwaysSalient span for named entities and dates are important. As this model eyes on OpenQA. It is important to let the model to focus on those named entities and\\xa0dates.Selecting the document from a larger corpus is important. The assumption is that the final result exist in extra documents. It is also important to pick top k related doucments.About MeI am Data Scientist in Bay Area. Focusing on state-of-the-art in Data Science, Artificial Intelligence\\xa0, especially in NLP and platform related. You can reach me from Medium Blog, LinkedIn or\\xa0Github.ReferenceP.Ram and A. G. Gray. Maximum Inner-Product Search using Tree Data-structures. 2012Devlin J., Chang M. W., Lee K. and Toutanova K.. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. 2018K. Guu, K. Lee, Z. Tung, P. Pasupat and M. W. Chang. REALM: Retrieval-Augmented Language Model Pre-Training. 2020REALM: Retrieval-Augmented Language Model Pre-Training was originally published in Towards AI\\u200a—\\u200aMultidisciplinary Science Journal on Medium, where people are continuing the conversation by highlighting and responding to this story.',\n",
       " 'Pretrained Transformer models have emerged as state-of-the-art approaches that learn contextual information from the text to improve the performance of several NLP tasks. Code: https://github.com/mulangonando/Impact-of-KG-Context-on-ED',\n",
       " 'Malaya is a Natural-Language-Toolkit library for Malay and Indonesian languages, powered by Deep Learning Tensorflow, https://malaya.readthedocs.io/en/latest/ Modules  Augmentation  Augment any text using dictionary of synonym, Wordvector or Transformer-Bahasa.  Constituency Parsing  Transfer learning on BERT-base-bahasa, Tiny-BERT-bahasa, Albert-base-bahasa, Albert-tiny-bahasa, XLNET-base-bahasa.  Dependency Parsing  Transfer learning on BERT-base-bahasa, Tiny-BERT-bahasa, Albert-base-bahasa, Albert-tiny-bahasa, XLNET-base-bahasa, ALXLNET-base-bahasa.  Emotion Analysis  Transfer learning on BERT-base-bahasa, Tiny-BERT-bahasa, Albert-base-bahasa, Albert-tiny-bahasa, XLNET-base-bahasa, ALXLNET-base-bahasa.  Entities Recognition  Transfer learning on BERT-base-bahasa, Tiny-BERT-bahasa, Albert-base-bahasa, Albert-tiny-bahasa, XLNET-base-bahasa, ALXLNET-base-bahasa.  Generator  Generate any texts given a context using T5-Bahasa, GPT2-Bahasa or Transformer-Bahasa.  Keyword Extraction  Provide RAKE, TextRank and Attention Mechanism hybrid with Transformer-Bahasa.  Language Detection  using Fast-text and Sparse Deep learning Model to classify Malay (formal and social media), Indonesia (formal and social media), Rojak language and Manglish.  Normalizer  using local Malaysia NLP researches hybrid with Transformer-Bahasa to normalize any bahasa texts.  Num2Word  Convert from numbers to cardinal or ordinal representation.  Paraphrase  Provide Abstractive Paraphrase using T5-Bahasa and Transformer-Bahasa.  Part-of-Speech Recognition  Transfer learning on BERT-base-bahasa, Tiny-BERT-bahasa, Albert-base-bahasa, Albert-tiny-bahasa, XLNET-base-bahasa, ALXLNET-base-bahasa.  Relevancy Analysis  Transfer learning on BERT-base-bahasa, Tiny-BERT-bahasa, Albert-base-bahasa, Albert-tiny-bahasa, XLNET-base-bahasa, ALXLNET-base-bahasa.  Sentiment Analysis  Transfer learning on BERT-base-bahasa, Tiny-BERT-bahasa, Albert-base-bahasa, Albert-tiny-bahasa, XLNET-base-bahasa, ALXLNET-base-bahasa.  Similarity  Using deep Encoder, Doc2Vec, BERT-base-bahasa, Tiny-BERT-bahasa, Albert-base-bahasa, Albert-tiny-bahasa, XLNET-base-bahasa and ALXLNET-base-bahasa to build deep semantic similarity models.  Spell Correction  Using local Malaysia NLP researches hybrid with Transformer-Bahasa to auto-correct any bahasa words.  Stemmer  Using BPE LSTM Seq2Seq with attention state-of-art to do Bahasa stemming.  Subjectivity Analysis  Transfer learning on BERT-base-bahasa, Tiny-BERT-bahasa, Albert-base-bahasa, Albert-tiny-bahasa, XLNET-base-bahasa, ALXLNET-base-bahasa.  Summarization  Provide Abstractive T5-Bahasa also Extractive interface using Transformer-Bahasa, skip-thought, LDA, LSA and Doc2Vec.  Topic Modelling  Provide Transformer-Bahasa, LDA2Vec, LDA, NMF and LSA interface for easy topic modelling with topics visualization.  Toxicity Analysis  Transfer learning on BERT-base-bahasa, Tiny-BERT-bahasa, Albert-base-bahasa, Albert-tiny-bahasa, XLNET-base-bahasa, ALXLNET-base-bahasa.  Transformer  Provide easy interface to load BERT-base-bahasa, Tiny-BERT-bahasa, Albert-base-bahasa, Albert-tiny-bahasa, XLNET-base-bahasa, ALXLNET-base-bahasa, ELECTRA-base-bahasa and ELECTRA-small-bahasa.  Translation  provide Neural Machine Translation using Transformer for EN to MS and MS to EN.  Word2Num  Convert from cardinal or ordinal representation to numbers.  Word2Vec  Provide pretrained bahasa wikipedia and bahasa news Word2Vec, with easy interface and visualization.  Zero-shot classification  Provide Zero-shot classification interface using Transformer-Bahasa to recognize texts without any labeled training data. Pretrained Models Malaya also released Bahasa pretrained models, simply check at https://github.com/huseinzol05/Malaya/tree/master/pretrained-model Or can try use huggingface 🤗 Transformers library, https://huggingface.co/models?filter=ms    submitted by    /u/huseinzol05   [link] [comments]',\n",
       " 'As a legal platform, Doctrine aggregates a lot of legal data with the intent of making them accessible, understandable and usable. The Machine Learning Engineers’ day-to-day material is mostly text: court decisions, legislation, legal commentaries, user queries, etc. All of our content is natural language, which we process in a number of ways: bag-of-words, embeddings or with language\\xa0models.In an ideal world though, our product would be built on top of scalable, flexible and reusable modules, ones that would be generic enough to accommodate a wide variety of legal contents and feed the whole spectrum of our product features. It is exactly with that vision in mind that we started working on a unified language model a few months ago, whose associated challenges, findings and results we’ll do our best to summarize in this\\xa0article.I. One language model to rule them\\xa0allDepending on the project, we were representing our legal contents\\xa0with:different techniques:TF-IDF vectorsBM25 (e.g., with ElasticSearch)A variant of Word2Vec, called Wang2Vec, embeddings fine-tuned on legal data\\u200a—\\u200anote that even if those embeddings work pretty well for a lot of tasks, they are not the state-of-the-art anymore. There’s not enough modeling power in simple word embeddings and we definitely see their limits now on some\\xa0tasks.2. different data:vocabulary of the content\\xa0itself,vocabulary of the linked contents from our legal\\xa0graphvocabulary from some metadata provided by the\\xa0courts…Yet eventually, we want to be able to represent all of our legal content using a unified framework for any text-understanding based feature, because\\xa0of:Reusability: all teams can rely on this unique language model for their projects.2. Scalability:a modeling power sufficient to be applied to any new legal content (e.g., legal documents from the lower house and the upper\\xa0house),robust enough to unlock use cases we’re not yet considering, like legal bots, legal trend detection, argument mining,\\xa0etc,generic enough to be applied to a new language (with a retraining on the new language of\\xa0course).3. Agnostic usage: one of the problems with our current representations is that the text follows some guidelines in the way they phrase statements, and a textual similarity is thus strongly biased towards documents that have the same overall phrasing (of the same court for example), despite the fact they’re not invoking the same laws about the same thing. For example, it is now difficult for us to match decisions from the High Court/Court of Appeal to those from the Supreme Court simply because of their different writing styles (the former tends to focus primarily and precisely on the facts, while the latter favors usually only relies on the legal matter, which has an adverse effect on our current representations).When we initially started thinking about this, there were some properties that we thought our language model should ideally\\xa0cover:Taking advantage of the semantic proximity:In French:préjudice corporel should be equivalent to dommage\\xa0corporelIn English: death should be equivalent to loss of\\xa0life2. Being able to represent our content on different granularities:Token-level for Named Entity Recognition: anonymization, entity detection,\\xa0…Paragraph-level: structure detection, argument similarities,\\xa0…Document-level: legal domain classification, document recommendation,\\xa0…It’s with all those things in mind that we started to work on a unique, all-encompassing language model serving all our use cases and features.II. Our legal language\\xa0modelThe first step of this project was to design the architecture and implementation of our language model. This step was crucial since it would serve as the foundation to all of our future work and help us move towards our initial vision. We first thought about our technical constraints:use an existing and robust implementation, in order to take advantage of the support and the community,use a state-of-the-art technique to achieve very good performances,ideally use a PyTorch implementation, because our previous Deep Learning algorithms were made with PyTorch. Moreover, PyTorch (along with a few others) remains the dominant deep learning library at the time of writing this\\xa0article,if possible, find an implementation with a French pre-trained model before fine-tuning, because transfer learning has shown its efficiency in\\xa0NLP.It should also be noted that compared to other use-cases, especially in academic research, the framework should be efficient at representing very long texts. Here is an interesting blog post about different document embeddings techniques. We’ll come to that\\xa0later.Under these constraints, the Hugging Face Transformers library appeared to be a very good\\xa0choice:they offer all the recent state-of-the-art architectures (BERT, RoBERTa, ELMo, XLNet,\\xa0…) complete with their associated PyTorch and TensorFlow implementations,some of them have a French pre-trained model,their implementation has quickly become an international reference, to the point where the famous NLP framework Spacy provides a Transformer implementation based on the Hugging Face\\xa0one.Among the models providing a French pre-trained model, we had the choice\\xa0between:BERT-Base, multilingualDistilmBERT, multilingualcamemBERT, French RoBERTa\\xa0modelWe decided to go for camemBERT, since it already provided good results for the French language on several tasks according to this paper. Of course, multilingual models will probably be very useful for internationalization later, but we initially wanted to check that a transformer model could be relevant. Moreover, camemBERT has fewer parameters than multilingual models, which makes it a little easier to\\xa0use.Note that camemBERT is case-sensitive, which will be useful for Named Entity Recognition and especially for anonymization.The legal CamemBERTNow that we had settled on the underlying technology, we decided to check how well it would perform on actual, real-life legal\\xa0data.Knowing that camemBERT was initially trained on the French subcorpus of OSCAR, which features gigabytes of data crawled from the web, we knew that it would fare well at general French language tasks, but we suspected that the task of speaking the more specific French legalese would prove to be a tougher nut to crack, which our initial tests confirmed.For example, when asked to predict the next word of the sentence Par ces\\xa0...\\xa0, camemBERT suggested the word mots, which is not exactly legal-oriented. We would expect something like moyens or\\xa0motifs.It was obvious at this point that the trove of millions of legal documents we have at our disposal at Doctrine would prove to be great material for the subsequent fine-tuning needed to harness the full power of our model. At this point, we were confident that the model could be trained, however, we needed it to be potentially used universally across features. Yet, one issue remained: how to handle long texts, a strong prerequisite for legal documents, but something that doesn’t pair naturally with transformers’ inherent limitations.BERT models, for example, have a hard limit of 512 to 514 tokens (as enforced by the max_position_embeddings parameter), which would surely be a challenge when dealing with court decisions: texts that can be infamously verbose, with an average token count hovering around 2000 (and some even more extreme cases like this decision).To circumvent this issue, we envisioned two different approaches:Embedding each paragraphHaving sliding windows, as explained hereTo avoid ending up with redundancy in the embeddings, we decided to go with paragraph embeddings first, with exceedingly long paragraphs getting snipped past the limit during training. What was left for us to determine at that point was an aggregation strategy over the different paragraphs, so that we could harvest the final document embeddings, something that we would come back to\\xa0later.We then proceeded with the implementation, which was done by splitting our legal documents on paragraphs and fine-tuning camemBERT on the masked language model task (using dedicated AWS GPU instances). It converged after a few days and we tested its relevance by using a few qualitative checks:Comparison between the standard pre-trained French camemBERT model and our legal camemBERT on a masked LM\\xa0taskWe assessed the differences in prediction for semantically similar sentences, which seemed to be consistent. The qualitative check seemed to provide very good results. It was now time to validate the language model on a real\\xa0task.III. Our first legal camemBERT use-case: classification of legal\\xa0domainWe wanted to try our legal camemBERT on a simple task for a first validation: text classification of legal domains on court decisions.This is indeed a simple and well delimited task, and easy to compare to other basic models. Moreover, this classification has a huge product impact, on the search filters, recommender systems and analytics.We have two hierarchies on the legal domains at Doctrine:the main legal\\xa0domain:Droit civil,Droit commercial,Droit social,Droit public,…2. the subdomain: for example in Droit civil, there\\xa0areDivorce et séparation de\\xa0corpsDroit locatifDroit des successionsDroit de la responsabilité…Today, we support 9 different domains and 40 different subdomains, where some are more complex than others to determine. These categories have a hierarchical structure, but we addressed the problem by reducing it to a 40-class classification problem.The HuggingFace repository suggests a classification head module integrated with CamemBERT. However, as discussed earlier, the main problem is that court decisions can be very verbose (have a look at this very long decision for example), and BERT does not work well on long texts. A very good review of document embeddings showed that there are no clear embedding technique that works better than others for very long documents. It really depends on your objective.Working at a paragraph level seemed more relevant, all the more so as the language model has been trained at a paragraph scale. BERT will then provide an embedding for each paragraph. We then had to think about a way to aggregate the paragraphs in order to get a decision embedding.ModelingParagraph embeddings methodIt is known that BERT architectures provide not only word-level contextual embeddings but also the special CLS-token whose output embedding is used for classification tasks. However it turns out to be a poor embedding of the input sequence of other tasks if not fine-tuned on the specific\\xa0task:The paper Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks from Reimers et al, 2019, shows that BERT out-of-the-box maps sentences to a vector space that is rather unsuitable to be used with common similarity measures like cosine-similarity.According to BERT creator Jacob Devlin: “I’m not sure what these vectors are, since BERT does not generate meaningful sentence vectors. It seems that this is is doing average pooling over the word tokens to get a sentence vector, but we never suggested that this will generate meaningful sentence representations.” sourceStill, the most classic ways to embed a document (in our case, a paragraph) with BERT\\xa0are:to use the [CLS]-tokento use an aggregation of the last X hidden states of the word embeddings ( we usually saw\\xa0X=4)What is interesting in our case is that one paragraph does not represent the whole court decision. We had to plug something on top of it. We decided to go with the [CLS]-token as paragraph embeddings for a first shot, because our task is a classification task.2. Document embedding with an aggregation over paragraphsGiven embeddings for all our paragraphs, we then had to think of a way to get document embeddings.Here again, different approaches can be considered, since this is another sequence-to-one vector modeling:A simple average of all paragraph embeddings (the [CLS]-token of each BERT-output paragraphs),A weighted average of the paragraph embeddings, with weights built with a self-attention mechanism explained in the paper A Structured Self-attentive Sentence Embedding,A bi-LSTM to exploit the sequential information contained in the paragraphs,A Convolutional Neural\\xa0Network,Another BERT that would learn the language at the paragraph scale,…Given that our task is a mere classification problem, the solution with a self-attention mechanism seemed to be pretty relevant for our case\\xa0because:It’s a bit smarter than a simple average-pooling, and it will automatically get rid of the useless paragraphs that contain no information for the legal domain. Indeed, the final paragraphs of French decisions are often related to the operative part of the judgment, and about who pays the costs. This is usually not relevant to our current\\xa0problem.It also provides some precious insights on how to best interpret the model. We can indeed have access to the attention weights and check on which paragraphs the model focused on the most for its prediction.With all that mind, here’s the final architecture for the classification task:Final architecture of our legal document classification on documents, using the legal camemBERTWe first tried to train the whole pipeline, including the fine-tuning of the legal camemBERT on this task, but we got memory errors. We quickly froze the BERT model and trained only the rest of the pipeline (attention layer + classification layer). It provided good results so we didn’t go with further experiments on an end-to-end training. This is something that we made a note of though, since unsupervised BERT outputs are known to be poor if not fine-tuned, as discussed earlier in this\\xa0article.ResultsThe goal here was not only to improve our legal domains classification, but also to show that we could achieve at least the same results as a simple TF-IDF\\xa0model.Dataset creationDeep learning in general often requires a consequent training set size. That’s why we used a semi-automatically labelled training dataset, labelled:by humans, using\\xa0Prodi.gywith business rules, using the associated court as a reference. If a decision is linked to another one from Labor court, it’s very likely that the decision is about Droit du travail(labor laws).with the most reliable predictions of our former algorithm, based on TF-IDF for the domain, and a legal taxonomy for the subdomain.Comparison between models and discussionWe achieved the same performance with our legal camemBERT and with a simple TF-IDF, which is actually good news! We indeed didn’t spend a lot of time on the modeling part of camemBERT, and this classification task is in the end a rather simple NLP\\xa0task.Moreover and perhaps just as interestingly, we noticed after a qualitative analysis of model’s prediction errors that the errors of the simple model were more often out of context. It means that when the TF-IDF gets it wrong, it’s really way off the mark. For example, this decision is predicted as Droit du transport with a probability of 0.96, instead of Droit des assurances because the decision is about a vehicle insurance claim and contains a lot of vocabulary related to transportation, and not that much about insurance.On the other hand, the legal camemBERT can of course be wrong, but it never steers too much out of context and will mostly predict subdomains that are very close, like Droit immobilier et de la construction and Droit de la copropriété et de la propriété immobilière, when we look at the confusion matrix.Moreover, CamemBERT managed to predict some subdomains that were not obvious at all, even for humans. For example, this decision has been predicted as Divorce et séparations de corpswithout any explicit mention of the word divorce in the decision! The subdomain here is very implicit and implied by a mention to a father that has to pay alimony to the mother of his\\xa0child.Let’s now have a look at the attention weights of our modeling. Here are some examples\\xa0below:Paragraph with the highest attention score (0.34) for the prediction of https://www.doctrine.fr/d/CA/Reims/2008/SK60FC7292250FC0B001E6 as Divorce et Séparation de\\xa0corpsParagraph with the highest attention score (0.26) for the prediction of https://www.doctrine.fr/d/CA/Rouen/2016/1F43DFAE32435B18DC90 as Droit des étrangers et de la nationalitéThese attention scores totally make sense, and confirmed the approach.We also confirmed that paragraphs related to generic procedures had a very low attention weight, like this\\xa0one:Paragraph with a very low attention weight of 0.01 for the prediction of https://www.doctrine.fr/d/CA/Rouen/2016/1F43DFAE32435B18DC90 as Droit des étrangers et de la nationalitéFinally, when we had a look at the errors of the models (both models), we quickly noticed that some classes were very well predicted and some others were not. Our intuition about the observed discrepancy boils down to the fact that language models are only ever as good as their training dataset. In our case, the issue seems to stem from volume and errors in the training set. This is definitely the next priority for this task to focus on, before trying to play with the different architectures. Indeed, the current one seems to work pretty well on subdomains when the training dataset is satisfactory.ConclusionWe built a legal language model with a state-of-the-art technique, that proved to be very efficient at capturing highly relevant information on a simple classification task. This is a huge step for Doctrine, as we have a lot of very complex tasks in Natural Language Processing to tackle! The granularity of this new language model, which can seamlessly provide token, paragraph and document embeddings will be key for us to find new applications for the technique on a wide array of complex Natural Language Processing tasks at Doctrine.In fact, the legal camemBERT has already found a second problem to tackle with the issue of semantic similarity between users and legal content in the context of a recommendation system and seems to already have yielded promising results, which we’ll be sharing in an upcoming blog post very soon. Stay\\xa0tuned!A single legal text representation at Doctrine: the legal camemBERT was originally published in Inside Doctrine on Medium, where people are continuing the conversation by highlighting and responding to this story.',\n",
       " 'The cleaned dataset and the pre-training models will facilitate the research of short-text conversation modeling. Code: https://github.com/thu-coai/CDial-GPT',\n",
       " 'Hey everyone, I am new to Natural Language Processing, but I have experience in Machine Learning and Convolutional Neural Network. While reading the GPT-3 paper, this question came to my mind, like having around 175 billion trainable the equation that will come out must be very complex and also it is trained on such a huge dataset. Than why is their no case of overfitting on this model.    submitted by    /u/psarangi112   [link] [comments]',\n",
       " 'Hello. I want to collect as many as papers as I can that will fall into this category. The main problem is that the \"tagging\" is not consistent for linguistic papers. Hence I\\'m looking for an exhausitve list of tags which are directly related to this field, in order to make better queries and find more relevant data.  Thanks!    submitted by    /u/quit_daedalus   [link] [comments]',\n",
       " 'Since the appearance of BERT, recent works including XLNet and RoBERTa utilize sentence embedding models pre-trained by large corpora and a large number of parameters. Code: https://github.com/snunlp/KR-BERT',\n",
       " 'Dialogue state modules are a useful component in a task-oriented dialogue system. Code: https://github.com/taolusi/dialogue-state-induction',\n",
       " 'Automatic text summarisation is the task of shortening the given piece of text yet retaining the main crux and flow. Check out the new paper explainer video 🔥  Check out - https://youtu.be/73uWfopdjoc Original Paper - https://www.aclweb.org/anthology/K17-1021/    submitted by    /u/prakhar21   [link] [comments]']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[doc.text for doc in raw_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>link</th>\n",
       "      <th>deep learning</th>\n",
       "      <th>natural language processing</th>\n",
       "      <th>computer vision</th>\n",
       "      <th>statistics</th>\n",
       "      <th>implementation</th>\n",
       "      <th>visualization</th>\n",
       "      <th>industry</th>\n",
       "      <th>software engineering</th>\n",
       "      <th>reddit question</th>\n",
       "      <th>arxiv</th>\n",
       "      <th>cloud computing</th>\n",
       "      <th>deployment</th>\n",
       "      <th>competitions</th>\n",
       "      <th>business</th>\n",
       "      <th>business intelligence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>REALM: Retrieval-Augmented Language Model Pre-Training</td>\n",
       "      <td>Natural Language ProcessingAn Introduction to Retrieval-Augmented Language Model Pre-TrainingPho...</td>\n",
       "      <td>https://medium.com/towards-artificial-intelligence/realm-retrieval-augmented-language-model-pre-...</td>\n",
       "      <td>0.594145</td>\n",
       "      <td>0.613702</td>\n",
       "      <td>0.534048</td>\n",
       "      <td>0.502438</td>\n",
       "      <td>0.564319</td>\n",
       "      <td>0.550380</td>\n",
       "      <td>0.517819</td>\n",
       "      <td>0.585611</td>\n",
       "      <td>0.549369</td>\n",
       "      <td>0.490301</td>\n",
       "      <td>0.518406</td>\n",
       "      <td>0.505397</td>\n",
       "      <td>0.519020</td>\n",
       "      <td>0.496123</td>\n",
       "      <td>0.518821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Evaluating the Impact of Knowledge Graph Context on Entity Disambiguation Models</td>\n",
       "      <td>Pretrained Transformer models have emerged as state-of-the-art approaches that learn contextual ...</td>\n",
       "      <td>https://paperswithcode.com/paper/evaluating-the-impact-of-knowledge-graph</td>\n",
       "      <td>0.613387</td>\n",
       "      <td>0.597722</td>\n",
       "      <td>0.531973</td>\n",
       "      <td>0.501481</td>\n",
       "      <td>0.593792</td>\n",
       "      <td>0.559076</td>\n",
       "      <td>0.545501</td>\n",
       "      <td>0.592912</td>\n",
       "      <td>0.543777</td>\n",
       "      <td>0.490492</td>\n",
       "      <td>0.511187</td>\n",
       "      <td>0.537554</td>\n",
       "      <td>0.530916</td>\n",
       "      <td>0.494104</td>\n",
       "      <td>0.496908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[P] Malay and Indonesian languages Toolkit, malaya.readthedocs.io</td>\n",
       "      <td>Malaya is a Natural-Language-Toolkit library for Malay and Indonesian languages, powered by Deep...</td>\n",
       "      <td>https://www.reddit.com/r/MachineLearning/comments/iav8g5/p_malay_and_indonesian_languages_toolkit/</td>\n",
       "      <td>0.573247</td>\n",
       "      <td>0.606518</td>\n",
       "      <td>0.518175</td>\n",
       "      <td>0.492254</td>\n",
       "      <td>0.534429</td>\n",
       "      <td>0.534387</td>\n",
       "      <td>0.504195</td>\n",
       "      <td>0.553177</td>\n",
       "      <td>0.534189</td>\n",
       "      <td>0.505804</td>\n",
       "      <td>0.495088</td>\n",
       "      <td>0.497745</td>\n",
       "      <td>0.496114</td>\n",
       "      <td>0.497304</td>\n",
       "      <td>0.506360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A single legal text representation at Doctrine: the legal camemBERT</td>\n",
       "      <td>As a legal platform, Doctrine aggregates a lot of legal data with the intent of making them acce...</td>\n",
       "      <td>https://medium.com/doctrine/a-single-legal-text-representation-at-doctrine-the-legal-camembert-a...</td>\n",
       "      <td>0.570231</td>\n",
       "      <td>0.586314</td>\n",
       "      <td>0.521751</td>\n",
       "      <td>0.499045</td>\n",
       "      <td>0.548911</td>\n",
       "      <td>0.511261</td>\n",
       "      <td>0.517473</td>\n",
       "      <td>0.595577</td>\n",
       "      <td>0.505991</td>\n",
       "      <td>0.492187</td>\n",
       "      <td>0.509414</td>\n",
       "      <td>0.505175</td>\n",
       "      <td>0.512034</td>\n",
       "      <td>0.511341</td>\n",
       "      <td>0.525712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Large-Scale Chinese Short-Text Conversation Dataset</td>\n",
       "      <td>The cleaned dataset and the pre-training models will facilitate the research of short-text conve...</td>\n",
       "      <td>https://paperswithcode.com/paper/a-large-scale-chinese-short-text-conversation</td>\n",
       "      <td>0.621184</td>\n",
       "      <td>0.586691</td>\n",
       "      <td>0.545824</td>\n",
       "      <td>0.541583</td>\n",
       "      <td>0.584535</td>\n",
       "      <td>0.550647</td>\n",
       "      <td>0.527896</td>\n",
       "      <td>0.600209</td>\n",
       "      <td>0.548811</td>\n",
       "      <td>0.484411</td>\n",
       "      <td>0.517256</td>\n",
       "      <td>0.519267</td>\n",
       "      <td>0.528229</td>\n",
       "      <td>0.519199</td>\n",
       "      <td>0.530539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[D] Why does models like GPT-3 or BERT don't have overfitting problems?</td>\n",
       "      <td>Hey everyone, I am new to Natural Language Processing, but I have experience in Machine Learning...</td>\n",
       "      <td>https://www.reddit.com/r/MachineLearning/comments/ib4rth/d_why_does_models_like_gpt3_or_bert_don...</td>\n",
       "      <td>0.574753</td>\n",
       "      <td>0.577933</td>\n",
       "      <td>0.525968</td>\n",
       "      <td>0.498621</td>\n",
       "      <td>0.520720</td>\n",
       "      <td>0.489520</td>\n",
       "      <td>0.512251</td>\n",
       "      <td>0.579364</td>\n",
       "      <td>0.521330</td>\n",
       "      <td>0.504109</td>\n",
       "      <td>0.526190</td>\n",
       "      <td>0.485646</td>\n",
       "      <td>0.517864</td>\n",
       "      <td>0.528094</td>\n",
       "      <td>0.550471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[Q] Data scientist here, working on gathering a corpus of academic papers focusing on \"Cognitive...</td>\n",
       "      <td>Hello. I want to collect as many as papers as I can that will fall into this category. The main ...</td>\n",
       "      <td>https://www.reddit.com/r/cognitivelinguistics/comments/ensu0n/q_data_scientist_here_working_on_g...</td>\n",
       "      <td>0.555300</td>\n",
       "      <td>0.578011</td>\n",
       "      <td>0.498356</td>\n",
       "      <td>0.489413</td>\n",
       "      <td>0.526189</td>\n",
       "      <td>0.513210</td>\n",
       "      <td>0.489906</td>\n",
       "      <td>0.544217</td>\n",
       "      <td>0.557324</td>\n",
       "      <td>0.480271</td>\n",
       "      <td>0.496461</td>\n",
       "      <td>0.485666</td>\n",
       "      <td>0.530694</td>\n",
       "      <td>0.496379</td>\n",
       "      <td>0.504614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>KR-BERT: A Small-Scale Korean-Specific Language Model</td>\n",
       "      <td>Since the appearance of BERT, recent works including XLNet and RoBERTa utilize sentence embeddin...</td>\n",
       "      <td>https://paperswithcode.com/paper/kr-bert-a-small-scale-korean-specific</td>\n",
       "      <td>0.574174</td>\n",
       "      <td>0.594593</td>\n",
       "      <td>0.522638</td>\n",
       "      <td>0.532562</td>\n",
       "      <td>0.583562</td>\n",
       "      <td>0.552562</td>\n",
       "      <td>0.547111</td>\n",
       "      <td>0.580898</td>\n",
       "      <td>0.551625</td>\n",
       "      <td>0.498453</td>\n",
       "      <td>0.515522</td>\n",
       "      <td>0.564181</td>\n",
       "      <td>0.550184</td>\n",
       "      <td>0.508991</td>\n",
       "      <td>0.513304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Dialogue State Induction Using Neural Latent Variable Models</td>\n",
       "      <td>Dialogue state modules are a useful component in a task-oriented dialogue system. Code: https://...</td>\n",
       "      <td>https://paperswithcode.com/paper/dialogue-state-induction-using-neural-latent</td>\n",
       "      <td>0.609547</td>\n",
       "      <td>0.595336</td>\n",
       "      <td>0.546996</td>\n",
       "      <td>0.492073</td>\n",
       "      <td>0.601822</td>\n",
       "      <td>0.568787</td>\n",
       "      <td>0.546473</td>\n",
       "      <td>0.602195</td>\n",
       "      <td>0.570310</td>\n",
       "      <td>0.499499</td>\n",
       "      <td>0.532563</td>\n",
       "      <td>0.543948</td>\n",
       "      <td>0.563158</td>\n",
       "      <td>0.529331</td>\n",
       "      <td>0.538693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[D] Supervised Approach to Extractive Text Summarisation | Research Paper Walkthrough</td>\n",
       "      <td>Automatic text summarisation is the task of shortening the given piece of text yet retaining the...</td>\n",
       "      <td>https://www.reddit.com/r/MachineLearning/comments/ibbo60/d_supervised_approach_to_extractive_text/</td>\n",
       "      <td>0.568026</td>\n",
       "      <td>0.569306</td>\n",
       "      <td>0.501154</td>\n",
       "      <td>0.508824</td>\n",
       "      <td>0.583346</td>\n",
       "      <td>0.544889</td>\n",
       "      <td>0.513401</td>\n",
       "      <td>0.552232</td>\n",
       "      <td>0.548748</td>\n",
       "      <td>0.499457</td>\n",
       "      <td>0.464308</td>\n",
       "      <td>0.529640</td>\n",
       "      <td>0.518805</td>\n",
       "      <td>0.491907</td>\n",
       "      <td>0.474505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                 title  \\\n",
       "0                                               REALM: Retrieval-Augmented Language Model Pre-Training   \n",
       "1                     Evaluating the Impact of Knowledge Graph Context on Entity Disambiguation Models   \n",
       "2                                    [P] Malay and Indonesian languages Toolkit, malaya.readthedocs.io   \n",
       "3                                  A single legal text representation at Doctrine: the legal camemBERT   \n",
       "4                                                A Large-Scale Chinese Short-Text Conversation Dataset   \n",
       "5                              [D] Why does models like GPT-3 or BERT don't have overfitting problems?   \n",
       "6  [Q] Data scientist here, working on gathering a corpus of academic papers focusing on \"Cognitive...   \n",
       "7                                                KR-BERT: A Small-Scale Korean-Specific Language Model   \n",
       "8                                         Dialogue State Induction Using Neural Latent Variable Models   \n",
       "9                [D] Supervised Approach to Extractive Text Summarisation | Research Paper Walkthrough   \n",
       "\n",
       "                                                                                                  text  \\\n",
       "0  Natural Language ProcessingAn Introduction to Retrieval-Augmented Language Model Pre-TrainingPho...   \n",
       "1  Pretrained Transformer models have emerged as state-of-the-art approaches that learn contextual ...   \n",
       "2  Malaya is a Natural-Language-Toolkit library for Malay and Indonesian languages, powered by Deep...   \n",
       "3  As a legal platform, Doctrine aggregates a lot of legal data with the intent of making them acce...   \n",
       "4  The cleaned dataset and the pre-training models will facilitate the research of short-text conve...   \n",
       "5  Hey everyone, I am new to Natural Language Processing, but I have experience in Machine Learning...   \n",
       "6  Hello. I want to collect as many as papers as I can that will fall into this category. The main ...   \n",
       "7  Since the appearance of BERT, recent works including XLNet and RoBERTa utilize sentence embeddin...   \n",
       "8  Dialogue state modules are a useful component in a task-oriented dialogue system. Code: https://...   \n",
       "9  Automatic text summarisation is the task of shortening the given piece of text yet retaining the...   \n",
       "\n",
       "                                                                                                  link  \\\n",
       "0  https://medium.com/towards-artificial-intelligence/realm-retrieval-augmented-language-model-pre-...   \n",
       "1                            https://paperswithcode.com/paper/evaluating-the-impact-of-knowledge-graph   \n",
       "2   https://www.reddit.com/r/MachineLearning/comments/iav8g5/p_malay_and_indonesian_languages_toolkit/   \n",
       "3  https://medium.com/doctrine/a-single-legal-text-representation-at-doctrine-the-legal-camembert-a...   \n",
       "4                       https://paperswithcode.com/paper/a-large-scale-chinese-short-text-conversation   \n",
       "5  https://www.reddit.com/r/MachineLearning/comments/ib4rth/d_why_does_models_like_gpt3_or_bert_don...   \n",
       "6  https://www.reddit.com/r/cognitivelinguistics/comments/ensu0n/q_data_scientist_here_working_on_g...   \n",
       "7                               https://paperswithcode.com/paper/kr-bert-a-small-scale-korean-specific   \n",
       "8                        https://paperswithcode.com/paper/dialogue-state-induction-using-neural-latent   \n",
       "9   https://www.reddit.com/r/MachineLearning/comments/ibbo60/d_supervised_approach_to_extractive_text/   \n",
       "\n",
       "   deep learning  natural language processing  computer vision  statistics  \\\n",
       "0       0.594145                     0.613702         0.534048    0.502438   \n",
       "1       0.613387                     0.597722         0.531973    0.501481   \n",
       "2       0.573247                     0.606518         0.518175    0.492254   \n",
       "3       0.570231                     0.586314         0.521751    0.499045   \n",
       "4       0.621184                     0.586691         0.545824    0.541583   \n",
       "5       0.574753                     0.577933         0.525968    0.498621   \n",
       "6       0.555300                     0.578011         0.498356    0.489413   \n",
       "7       0.574174                     0.594593         0.522638    0.532562   \n",
       "8       0.609547                     0.595336         0.546996    0.492073   \n",
       "9       0.568026                     0.569306         0.501154    0.508824   \n",
       "\n",
       "   implementation  visualization  industry  software engineering  \\\n",
       "0        0.564319       0.550380  0.517819              0.585611   \n",
       "1        0.593792       0.559076  0.545501              0.592912   \n",
       "2        0.534429       0.534387  0.504195              0.553177   \n",
       "3        0.548911       0.511261  0.517473              0.595577   \n",
       "4        0.584535       0.550647  0.527896              0.600209   \n",
       "5        0.520720       0.489520  0.512251              0.579364   \n",
       "6        0.526189       0.513210  0.489906              0.544217   \n",
       "7        0.583562       0.552562  0.547111              0.580898   \n",
       "8        0.601822       0.568787  0.546473              0.602195   \n",
       "9        0.583346       0.544889  0.513401              0.552232   \n",
       "\n",
       "   reddit question     arxiv  cloud computing  deployment  competitions  \\\n",
       "0         0.549369  0.490301         0.518406    0.505397      0.519020   \n",
       "1         0.543777  0.490492         0.511187    0.537554      0.530916   \n",
       "2         0.534189  0.505804         0.495088    0.497745      0.496114   \n",
       "3         0.505991  0.492187         0.509414    0.505175      0.512034   \n",
       "4         0.548811  0.484411         0.517256    0.519267      0.528229   \n",
       "5         0.521330  0.504109         0.526190    0.485646      0.517864   \n",
       "6         0.557324  0.480271         0.496461    0.485666      0.530694   \n",
       "7         0.551625  0.498453         0.515522    0.564181      0.550184   \n",
       "8         0.570310  0.499499         0.532563    0.543948      0.563158   \n",
       "9         0.548748  0.499457         0.464308    0.529640      0.518805   \n",
       "\n",
       "   business  business intelligence  \n",
       "0  0.496123               0.518821  \n",
       "1  0.494104               0.496908  \n",
       "2  0.497304               0.506360  \n",
       "3  0.511341               0.525712  \n",
       "4  0.519199               0.530539  \n",
       "5  0.528094               0.550471  \n",
       "6  0.496379               0.504614  \n",
       "7  0.508991               0.513304  \n",
       "8  0.529331               0.538693  \n",
       "9  0.491907               0.474505  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
